{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tsumugi","text":"<p>A modern PySpark Connect/Classic wrapper on top of the Deequ, a beautiful Data Quality  library from AWS Labs.</p> <p></p> <p>NOTE: Tsumugi Shiraui is a chimera: a hybrid of Human and Gauna. She combines the chaotic power of Gauna with a Human intillegence and empathia. Like an original character of the Manga \"Knights of Sidonia\", this project aims to make a hybrid of very powerful but hard to learn and use Deequ Scala Library with a usability and simplicity of Spark Connect (PySpark Connect, Spark Connect Go, Spark Connect Rust, etc.).</p>"},{"location":"#table-on-contents","title":"Table on contents","text":"<ul> <li>Lungauge agnostic concepts and usecases for Deequ<ul> <li>Concepts of Deequ</li> <li>Possible usecases for Deequ</li> </ul> </li> <li>PySpark Connect / Classic API<ul> <li>Main data structures and classes</li> <li>API Docs (auto-generated)</li> </ul> </li> <li>Example notebooks<ul> <li>Basic example</li> <li>Using predicates with analyzers</li> </ul> </li> </ul>"},{"location":"gen_ref_pages/","title":"Gen ref pages","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Generate the code reference pages and navigation.\n\nScript was taken from\nhttps://mkdocstrings.github.io/recipes/#automatic-code-reference-pages\n\"\"\"\n</pre> \"\"\"Generate the code reference pages and navigation.  Script was taken from https://mkdocstrings.github.io/recipes/#automatic-code-reference-pages \"\"\" In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>import mkdocs_gen_files\n</pre> import mkdocs_gen_files In\u00a0[\u00a0]: Copied! <pre>nav = mkdocs_gen_files.Nav()\n</pre> nav = mkdocs_gen_files.Nav() In\u00a0[\u00a0]: Copied! <pre>for path in sorted(Path(\".\").rglob(\"tsumugi/**/*.py\")):\n    if \"proto\" in path.absolute().__str__():\n        # We do not need to expose generated code\n        continue\n    module_path = path.relative_to(\".\").with_suffix(\"\")\n    doc_path = path.relative_to(\".\").with_suffix(\".md\")\n    full_doc_path = Path(\"python/reference\", doc_path)\n\n    parts = tuple(module_path.parts)\n\n    if parts[-1] == \"__init__\":\n        parts = parts[:-1]\n        doc_path = doc_path.with_name(\"index.md\")\n        full_doc_path = full_doc_path.with_name(\"index.md\")\n    elif parts[-1] == \"__main__\":\n        continue\n\n    nav[parts] = doc_path.as_posix()  #\n\n    with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:\n        ident = \".\".join(parts)\n        fd.write(f\"::: {ident}\")\n\n    mkdocs_gen_files.set_edit_path(full_doc_path, path)\n</pre> for path in sorted(Path(\".\").rglob(\"tsumugi/**/*.py\")):     if \"proto\" in path.absolute().__str__():         # We do not need to expose generated code         continue     module_path = path.relative_to(\".\").with_suffix(\"\")     doc_path = path.relative_to(\".\").with_suffix(\".md\")     full_doc_path = Path(\"python/reference\", doc_path)      parts = tuple(module_path.parts)      if parts[-1] == \"__init__\":         parts = parts[:-1]         doc_path = doc_path.with_name(\"index.md\")         full_doc_path = full_doc_path.with_name(\"index.md\")     elif parts[-1] == \"__main__\":         continue      nav[parts] = doc_path.as_posix()  #      with mkdocs_gen_files.open(full_doc_path, \"w\") as fd:         ident = \".\".join(parts)         fd.write(f\"::: {ident}\")      mkdocs_gen_files.set_edit_path(full_doc_path, path) In\u00a0[\u00a0]: Copied! <pre>with mkdocs_gen_files.open(\"python/reference/SUMMARY.md\", \"w\") as nav_file:\n    nav_file.writelines(nav.build_literate_nav())\n</pre> with mkdocs_gen_files.open(\"python/reference/SUMMARY.md\", \"w\") as nav_file:     nav_file.writelines(nav.build_literate_nav())"},{"location":"concepts/concepts/","title":"Concepts","text":"<p>Since the project is primarily about creating a wrapper, it adheres to the same concepts and ideas that underpin the original Deequ library.</p> <p>You can read more about that in the publications by the authors.</p> <ul> <li>Schelter, Sebastian, et al. \"Automating large-scale data quality verification.\" Proceedings of the VLDB Endowment 11.12 (2018): 1781-1794., link;</li> <li>Schelter, Sebastian, et al. \"Unit testing data with deequ.\" Proceedings of the 2019 International Conference on Management of Data. 2019., link;</li> <li>Schelter, Sebastian, et al. \"Deequ-data quality validation for machine learning pipelines.\" (2018)., link;</li> </ul>"},{"location":"concepts/concepts/#analyzers","title":"Analyzers","text":"<p>Analyzers in Deequ are similar to data metrics. Some of them are quite simple and can actually be computed using the Spark SQL API, such as <code>Size</code>, which is simply the number of rows in a <code>DataFrame</code>. However, some analyzers, like <code>Entropy</code> or <code>KLLSketch</code>, are more complex to compute. Behind the scenes, Deequ combines all the analyzers into custom accumulators and computes results at scale using low-level Apache Spark APIs. End users shouldn't need to worry about the execution model, but they should understand the range of metrics that can be computed.</p> <p>At the moment this library supports the following analyzers:</p> <ul> <li><code>ApproxCountDistinct</code>: Computes the approximate count of distinct values in the data; recommended over <code>CountDistinct</code> for large datasets.</li> <li><code>ApproxQuantile</code>: Computes the approximate quantile of a given numeric column.</li> <li><code>ApproxQuantiles</code>: Computes quantiles for a set of columns.</li> <li><code>ColumnCount</code>: Returns the number of columns in the data.</li> <li><code>Completeness</code>: Calculates the fraction of non-null values in a column.</li> <li><code>Compliance</code>: Calculates the fraction of rows that satisfy a given predicate.</li> <li><code>Correlation</code>: Computes the correlation between two columns.</li> <li><code>CountDistinct</code>: Calculates the exact count of distinct values for a given set of columns.</li> <li><code>CustomSql</code>: Computes a metric based on a provided custom SQL snippet.</li> <li><code>Distinctness</code>: Calculates the fraction of distinct values in a column.</li> <li><code>Entropy</code>: Measures the level of information contained in a message. Given the probability distribution of values in a column, it describes how many bits are required to identify a value.</li> <li><code>ExactQuantile</code>: Computes the exact value of a given quantile for a specific column.</li> <li><code>Histogram</code>: Summarizes values in a DataFrame column. Groups the column's values and calculates either the number of rows with a specific value or the fraction of this value, or sums values in another column.</li> <li><code>KLLSketch</code>: Computes quantiles sketch (see Zohar Karnin, Kevin Lang, Edo Liberty, arXiv 2016 for details).</li> <li><code>MaxLength</code>: Calculates the maximum length of values in a given column.</li> <li><code>Maximum</code>: Computes the maximum value.</li> <li><code>Mean</code>: Calculates the average.</li> <li><code>MinLength</code>: Determines the minimum length of values in a given column.</li> <li><code>Minimum</code>: Computes the minimum value.</li> <li><code>MutualInformation</code>: Calculates the Mutual Information (MI) metric, which describes how much information about one column can be inferred from another. MI is zero for independent columns and equals the entropy of each column for functionally dependent columns.</li> <li><code>PatternMatch</code>: Counts the number of rows that match a given regular expression pattern.</li> <li><code>RatioOfSums</code>: Computes the ratio of sums between two columns.</li> <li><code>Size</code>: Calculates the number of rows.</li> <li><code>StandardDeviation</code>: Computes the standard deviation of a given numeric column.</li> <li><code>Sum</code>: Calculates the sum.</li> <li><code>UniqueValueRatio</code>: Computes the ratio of unique values (number of unique values divided by the total number of rows).</li> <li><code>Uniqueness</code>: Same as <code>UniqueValueRatio</code>.</li> </ul> <p>Most of analyzers also accept an optional parameter <code>where</code> that allows to compute metrics using predicate.</p>"},{"location":"concepts/concepts/#checks","title":"Checks","text":"<p>Each thing in Deequ is <code>Check</code>. Check is a named set of data quality rules that should be applyed on data. Check can contain name, description, required analyzers and one or more constraints.</p>"},{"location":"concepts/concepts/#required-analyzers","title":"Required analyzers","text":"<p>Required analyzers are those that will be computed regardless of other factors. This feature can be useful when one does not need to use constraints and wants to utilize Deequ solely as a data profiler.</p>"},{"location":"concepts/concepts/#constraints","title":"Constraints","text":"<p>A constraint in Deequ is a combination of an analyzer and an assertion about an expected value. While Deequ allows users to pass a function <code>metric -&gt; boolean</code> as an assertion, Tsumugi does not provide this feature due to problems with serializing lambda functions. Instead, constraints in Tsumugi contain the following information:</p> <ul> <li>analyzer itself;</li> <li>an expected value of the metric;</li> <li>a comparison sign (<code>&lt;</code>, <code>&lt;=</code>, <code>==</code>, <code>&gt;</code>, <code>&gt;=</code>);</li> </ul> <p>I found that method of building assertions to be the most robust, and it is entirely sufficient for all my own use cases. However, if you're interested in more advanced assertions, please feel free to open an issue describing your specific use case, and I will try to implement it.</p>"},{"location":"concepts/concepts/#anomaly-detection","title":"Anomaly Detection","text":"<p>Deequ not only supports computing static constraints with pre-defined assertions and analyzers but also offers anomaly detection capabilities. Under the hood, Deequ can store all computed metrics in a repository. This feature allows users to define constraints such as \"the average value of my column should not change by more than \u00b15% between different data batches.\"</p> <p>Under the hood, Deequ uses a chosen comparison strategy to analyze historical data of computed metrics to determine if new data is anomalous. This is especially important in Machine Learning (ML) applications because it's quite challenging to define static constraints for ML models. These models often have non-trivial underlying logic, such as normalization and standardization. It's crucial to run ML models on data that is similar to the data on which they were trained.</p>"},{"location":"concepts/concepts/#analyzers-that-can-be-used-with-ad","title":"Analyzers that can be used with AD","text":"<p>At present, only analyzers that compute <code>Double</code> or <code>Long</code> metrics can be used with Anomaly Detection. Most analyzers fit these requirements, except for <code>Histogram</code>, <code>KLLSketch</code>, and a few others.</p>"},{"location":"concepts/concepts/#anomaly-check-config","title":"Anomaly Check Config","text":"<p>One can use <code>AnomalyCheckConfig</code> object that provides the following available options:</p> <ul> <li><code>level</code>: Severity level of the AD Check, the same as in Checks;</li> <li><code>description</code>: A string description of the AD Check;</li> <li><code>tags</code>: A map of <code>string -&gt; string</code> that may contain any tags you want to add to the repository;</li> <li><code>after_date</code>: A <code>Long</code> (or <code>int</code> in Python) value that filters out all keys before it when determining if the case is an anomaly or not;</li> <li><code>before_date</code>: A <code>Long</code> (or <code>int</code> in Python) value that filters out all keys after it when determining if the case is an anomaly or not;</li> </ul> <p>NOTE: Due to the internal implementation details of upstream Deequ, running your AD suite twice on the same data with the same <code>ResultKey</code> will cause it to compare the latest run with the previous run, rather than with a run using a different key. To avoid this, simply pass a value lower than the current <code>result_key</code> to your configuration.</p>"},{"location":"concepts/concepts/#repositories","title":"Repositories","text":"<p>At the moment the following Metric Repositories are supported:</p> <ul> <li><code>FileSystemMetricRepository</code> that stores historical data about metrics in JSON files in any supported by <code>Hadoop</code> file system (Local FS, S3, MinIO, HDFS, etc.);</li> <li><code>SparkTableMetricRepository</code> that stores historical data in the spark table, registered in the current-session Catalog;</li> </ul>"},{"location":"concepts/concepts/#result-key","title":"Result Key","text":"<p><code>ResultKey</code> is an object that defines the key and tags of the run. The most straightforward solution would be to use a timestamp derived from either the execution date or the date corresponding to the checked data. For example, an excellent choice would be to use the timestamp value of the partition for data that is partitioned by load date.</p>"},{"location":"concepts/concepts/#strategies","title":"Strategies","text":"<p>The following strategies are supported at the moment:</p> <ul> <li><code>AbsoluteChangeStrategy</code>: Uses static constraints on top of absulute changes of the metrics;</li> <li><code>BatchNormalStrategy</code>: Detects anomalies based on the mean and standard deviation of all available values. Assumes that the data is normally distributed;</li> <li><code>OnlineNormalStrategy</code>: Detects anomalies based on the running mean and standard deviation. Anomalies can be excluded from the computation to not affect the calculated mean/ standard deviation. Assumes that the data is normally distributed;</li> <li><code>RelativeRateOfChangeStrategy</code>: Detects anomalies based on the values' rate of change. The order of the difference can be set manually. If it is set to 0, this strategy acts like the <code>SimpleThresholdStrategy</code>;</li> <li><code>SimpleThresholdStrategy</code>: A simple anomaly detection method that checks if values are in a specified range;</li> </ul>"},{"location":"concepts/concepts/#verification-suite","title":"Verification Suite","text":"<p>The suite in Deequ consists of:</p> <ul> <li>Checks;</li> <li>Anomaly Detection cases;</li> <li>Required analyzers;</li> <li>Repository;</li> </ul> <p>It serves as the main entry point for creating a Deequ job.</p>"},{"location":"concepts/usecases/","title":"Usecases for Deequ","text":"<p>Compared to other data quality tools that primarily follow zero-code or low-code paradigms, Deequ is a code-first solution that provides a well-designed and stable programming API. This approach makes Deequ highly flexible, allowing you to design your own YAML-like low-code API with a structure that fits your specific domain. In essence, Deequ functions more as a data quality engine, adaptable to most possible use cases. Some of these use cases are described below.</p>"},{"location":"concepts/usecases/#data-profiling","title":"Data Profiling","text":"<p>The first and most obvious use case for Deequ is data profiling. In this scenario, one does not need to use <code>Check</code>, <code>Constraint</code>, or <code>AnomalyDetection</code> features. It would be sufficient to simply add all the analyzers to the <code>required_analyzers</code> section of the <code>VerificationSuite</code>. This approach would not produce any check results or row-level results, but instead generate a simple table with computed metrics per instance (which may be a <code>Column</code> or <code>Dataset</code>).</p> <p>As a result, you will obtain a list of metrics and their corresponding values. Since both Deequ and Tsumugi are code-first (rather than YAML-first) frameworks, it will be very easy to customize your profiling based on the data types in your <code>DataFrame</code>.</p>"},{"location":"concepts/usecases/#static-constraints-based-on-the-business-rules","title":"Static constraints based on the business rules","text":"<p>The next potential use case for Deequ is to add static constraints to your tables and use row-level results to quarantine data that fails to meet these constraints. For example, if you have a string column in one of your tables that should always contain exactly 14 characters, such as a mobile phone number, you can add a constraint specifying that both <code>MaxLength</code> and <code>MinLength</code> should be exactly 14. You can then use row-level results to identify which rows passed the constraint and which did not. These row-level results will contain your data along with one boolean column for each <code>Check</code>, indicating whether the row passed all the constraints in that <code>Check</code> or not. Another good option might be the <code>PatternMatch</code> analyzer, which could be used to check if a column contains a valid email address and quarantine the row if it doesn't.</p>"},{"location":"concepts/usecases/#detecting-data-drift-for-ml-inference","title":"Detecting data-drift for ML-inference","text":"<p>Another excellent use case for Deequ is as a data drift detector for checking input in ML model batch inference. Imagine we have an ML-based recommender system that updates pre-computed recommendations daily for our users for the following day. This scenario is a good example of batch inference, where we have an ML model trained once on training data and run it each day offline on new data. For such a system, it is crucial to ensure that the data hasn't changed significantly compared to previous batches. If it has, it signals that our ML model should be retrained on more recent training data.</p> <p>As we can see, there are no static constraints here. Rather than fitting our data into strict boundaries, we aim to ensure that data drift remains within acceptable limits.</p> <p>This scenario presents a perfect use case for Deequ Anomaly Detection. Let's imagine we have an ML model trained on the following features:</p> <ol> <li>Duration of customer relationship (numeric)</li> <li>Paid subscription status (boolean, can be NULL)</li> <li>Frequency of service usage (numeric)</li> </ol> <p>In this case, we can apply the following Anomaly Detection checks:</p> <ul> <li>Assuming that the average, minimum, and maximum frequency of service usage should not change dramatically, we can apply a <code>RelativeRateOfChange</code> strategy. By setting maximum increase and minimum decrease values to 1.1 and 0.9 respectively, we allow for a \u00b110% drift. Any new batch that shows significant changes compared to the previous batch of data will be considered an anomaly in this case.</li> <li>Because our model uses a missing value imputation strategy to fill NULLs in a flag column, we need to ensure that the amount of NULLs is similar to the data on which we trained our ML model. For this case, a <code>SimpleThresholdStrategy</code> is a good choice: we can set maximum and minimum allowed drift limits, and any data that falls within this range will be considered acceptable. Regarding the frequency of service usage, we know that the value should approximate a Normal Distribution. This means we can apply the <code>BatchNormalStrategy</code> to our batch intervals and ensure that the data is actually normally distributed by using thresholds for the mean and standard deviation of metrics.</li> </ul>"},{"location":"notebooks/basic_example/","title":"Basic usage","text":"In\u00a0[1]: Copied! <pre>from pyspark.sql import SparkSession\n</pre> from pyspark.sql import SparkSession In\u00a0[2]: Copied! <pre>spark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n</pre> spark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate() In\u00a0[3]: Copied! <pre>test_data = spark.createDataFrame(\n    [\n        [\"foo\", 1, 4],\n        [\"bar\", 2, 6],\n        [\"baz\", 3, None],\n    ],\n    schema=\"struct&lt;a:string, b:int, c:int&gt;\"\n)\n</pre> test_data = spark.createDataFrame(     [         [\"foo\", 1, 4],         [\"bar\", 2, 6],         [\"baz\", 3, None],     ],     schema=\"struct\" ) In\u00a0[4]: Copied! <pre>test_data.show()\n</pre> test_data.show() <pre>+---+---+----+\n|  a|  b|   c|\n+---+---+----+\n|foo|  1|   4|\n|bar|  2|   6|\n|baz|  3|NULL|\n+---+---+----+\n\n</pre> In\u00a0[5]: Copied! <pre>from tsumugi.verification import VerificationSuite\nfrom tsumugi.analyzers import Size, Minimum, Completeness, CustomSql, ConstraintBuilder\nfrom tsumugi.checks import CheckBuilder\n</pre> from tsumugi.verification import VerificationSuite from tsumugi.analyzers import Size, Minimum, Completeness, CustomSql, ConstraintBuilder from tsumugi.checks import CheckBuilder In\u00a0[6]: Copied! <pre>suite = (\n    VerificationSuite\n    .on_data(test_data) # add DataFrame\n    .with_row_level_results() # mark that row-level results are required\n    .add_check(\n        CheckBuilder()\n        .with_description(\"Basic checks\")\n        .has_size(expected_size=3, name=\"Size(*)\")\n        .is_primary_key(column=\"b\", name=\"col(b) should be PK-like\")\n        .build()\n    )\n    .add_check(\n        CheckBuilder()\n        .with_description(\"Additional checks\")\n        .is_complete(column=\"c\", name=\"col(c) should be complete\")\n        .with_constraint(\n            ConstraintBuilder()\n            .for_analyzer(Minimum(column=\"b\"))\n            .should_be_eq_to(0.0)\n            .build()\n        )\n        .build()\n    )\n)\n</pre> suite = (     VerificationSuite     .on_data(test_data) # add DataFrame     .with_row_level_results() # mark that row-level results are required     .add_check(         CheckBuilder()         .with_description(\"Basic checks\")         .has_size(expected_size=3, name=\"Size(*)\")         .is_primary_key(column=\"b\", name=\"col(b) should be PK-like\")         .build()     )     .add_check(         CheckBuilder()         .with_description(\"Additional checks\")         .is_complete(column=\"c\", name=\"col(c) should be complete\")         .with_constraint(             ConstraintBuilder()             .for_analyzer(Minimum(column=\"b\"))             .should_be_eq_to(0.0)             .build()         )         .build()     ) ) In\u00a0[7]: Copied! <pre>result = suite.run()\n</pre> result = suite.run() In\u00a0[8]: Copied! <pre>result.check_results_as_pandas()\n</pre> result.check_results_as_pandas() Out[8]: level check_description constraint_message metric_name metric_instance metric_entity metric_value status constraint 0 Warning Basic checks Size * Dataset 3.000000 Success SizeConstraint(Size(None)) 1 Warning Basic checks Uniqueness b Column 1.000000 Success UniquenessConstraint(Uniqueness(Buffer(b),None... 2 Warning Basic checks Completeness b Column 1.000000 Success CompletenessConstraint(Completeness(b,None,Som... 3 Warning Additional checks Value: 0.6666666666666666 does not meet the co... Completeness c Column 0.666667 Failure CompletenessConstraint(Completeness(c,None,Som... 4 Warning Additional checks Value: 1.0 does not meet the constraint requir... Minimum b Column 1.000000 Failure MinimumConstraint(Minimum(b,None,Some(Analyzer... In\u00a0[9]: Copied! <pre>result.checks_as_pandas()\n</pre> result.checks_as_pandas() Out[9]: check check_level check_status constraint constraint_status constraint_message 0 Basic checks Warning Success SizeConstraint(Size(None)) Success 1 Basic checks Warning Success UniquenessConstraint(Uniqueness(Buffer(b),None... Success 2 Basic checks Warning Success CompletenessConstraint(Completeness(b,None,Som... Success 3 Additional checks Warning Warning CompletenessConstraint(Completeness(c,None,Som... Failure Value: 0.6666666666666666 does not meet the co... 4 Additional checks Warning Warning MinimumConstraint(Minimum(b,None,Some(Analyzer... Failure Value: 1.0 does not meet the constraint requir... In\u00a0[10]: Copied! <pre>result.metrics_as_pandas()\n</pre> result.metrics_as_pandas() Out[10]: entity instance name value 0 Column c Completeness 0.666667 1 Column b Uniqueness 1.000000 2 Dataset * Size 3.000000 3 Column b Completeness 1.000000 4 Column b Minimum 1.000000 In\u00a0[11]: Copied! <pre>result.row_level_results.toPandas()\n</pre> result.row_level_results.toPandas() Out[11]: a b c Basic checks Additional checks 0 foo 1 4.0 True False 1 bar 2 6.0 True False 2 baz 3 NaN True False"},{"location":"notebooks/basic_example/#basic-usage","title":"Basic usage\u00b6","text":"<p>This notebook contains a minimal example of how Tsumugi can be used with PySpark Connect.</p>"},{"location":"notebooks/using_where/","title":"Using predicates","text":"In\u00a0[1]: Copied! <pre>from pyspark.sql import SparkSession\nimport pandas as pd\n</pre> from pyspark.sql import SparkSession import pandas as pd In\u00a0[2]: Copied! <pre>spark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n</pre> spark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate() In\u00a0[3]: Copied! <pre>iris = spark.createDataFrame(pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/iris.csv\"))\n</pre> iris = spark.createDataFrame(pd.read_csv(\"https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/iris.csv\")) In\u00a0[4]: Copied! <pre>iris.show()\n</pre> iris.show() <pre>+------------+-----------+------------+-----------+-------+\n|sepal_length|sepal_width|petal_length|petal_width|species|\n+------------+-----------+------------+-----------+-------+\n|         5.1|        3.5|         1.4|        0.2| setosa|\n|         4.9|        3.0|         1.4|        0.2| setosa|\n|         4.7|        3.2|         1.3|        0.2| setosa|\n|         4.6|        3.1|         1.5|        0.2| setosa|\n|         5.0|        3.6|         1.4|        0.2| setosa|\n|         5.4|        3.9|         1.7|        0.4| setosa|\n|         4.6|        3.4|         1.4|        0.3| setosa|\n|         5.0|        3.4|         1.5|        0.2| setosa|\n|         4.4|        2.9|         1.4|        0.2| setosa|\n|         4.9|        3.1|         1.5|        0.1| setosa|\n|         5.4|        3.7|         1.5|        0.2| setosa|\n|         4.8|        3.4|         1.6|        0.2| setosa|\n|         4.8|        3.0|         1.4|        0.1| setosa|\n|         4.3|        3.0|         1.1|        0.1| setosa|\n|         5.8|        4.0|         1.2|        0.2| setosa|\n|         5.7|        4.4|         1.5|        0.4| setosa|\n|         5.4|        3.9|         1.3|        0.4| setosa|\n|         5.1|        3.5|         1.4|        0.3| setosa|\n|         5.7|        3.8|         1.7|        0.3| setosa|\n|         5.1|        3.8|         1.5|        0.3| setosa|\n+------------+-----------+------------+-----------+-------+\nonly showing top 20 rows\n\n</pre> In\u00a0[5]: Copied! <pre>from tsumugi.verification import VerificationSuite\nfrom tsumugi.analyzers import Mean\nfrom tsumugi.checks import CheckBuilder\n</pre> from tsumugi.verification import VerificationSuite from tsumugi.analyzers import Mean from tsumugi.checks import CheckBuilder In\u00a0[6]: Copied! <pre>suite = (\n    VerificationSuite\n    .on_data(iris)\n    .add_required_analyzer(\n        Mean(column=\"sepal_length\", where=\"species = 'setosa'\")\n    )\n    .add_required_analyzer(\n        Mean(column=\"sepal_length\", where=\"species = 'versicolor'\")\n    )\n    .add_required_analyzer(\n        Mean(column=\"sepal_length\", where=\"species = 'virginica'\")\n    )\n)\n</pre> suite = (     VerificationSuite     .on_data(iris)     .add_required_analyzer(         Mean(column=\"sepal_length\", where=\"species = 'setosa'\")     )     .add_required_analyzer(         Mean(column=\"sepal_length\", where=\"species = 'versicolor'\")     )     .add_required_analyzer(         Mean(column=\"sepal_length\", where=\"species = 'virginica'\")     ) ) In\u00a0[7]: Copied! <pre>result = suite.run()\n</pre> result = suite.run() In\u00a0[8]: Copied! <pre>result.metrics_as_pandas()\n</pre> result.metrics_as_pandas() Out[8]: entity instance name value 0 Column sepal_length Mean (where: species = 'setosa') 5.006 1 Column sepal_length Mean (where: species = 'versicolor') 5.936 2 Column sepal_length Mean (where: species = 'virginica') 6.588"},{"location":"notebooks/using_where/#using-predicates","title":"Using predicates\u00b6","text":"<p>This example show how the <code>where</code> option can be used with analyzers.</p>"},{"location":"python/main_structures/","title":"Main Data Structures and classes","text":"<p>The Tsumugi project is built on top of classes generated by the protobuf compiler from protobuf messages. While the generated code is part of the Tsumugi python-client distribution, it is strongly recommended not to use these classes directly. From a developer's perspective, protobuf messages should be considered part of the internal API, not the public API. I cannot provide any guarantees that these messages will always remain backward compatible.</p>"},{"location":"python/main_structures/#analyzers","title":"Analyzers","text":"<p>Each analyzer in the python-client is at the same moment a <code>dataclass</code> but also it inherits a parent class <code>AbstractAnalyzer</code>.</p> <pre><code>  class AbstractAnalyzer(ABC):\n      \"\"\"Abstract class for all analyzers in tsumugi.\"\"\"\n\n      @abstractmethod\n      def _to_proto(self) -&gt; proto.Analyzer: ...\n</code></pre> <p>The majority of Tusumugi's python-client code serves as a thin facade over generated code. As a result, there is minimal implementation: each analyzer contains only the information necessary to create a corresponding protobuf message. The only private method is <code>_to_proto(self) -&gt; proto.Analyzer</code>, creates a message from the fields of the dataclass.</p>"},{"location":"python/main_structures/#analyzer-options","title":"Analyzer Options","text":"<p>Most of the Deequ analyzers accept an <code>AnalyzerOption</code> object as an optional argument. This object defines how the analyzer should handle NULL values in the checked columns and how it should operate if the user requests row-level results.</p> <pre><code>  @dataclass\n  class AnalyzerOptions:\n      \"\"\"Container for Analyzer Options.\"\"\"\n\n      null_behaviour: NullBehaviour\n      filtered_row_outcome: FilteredRowOutcome\n</code></pre> <p><code>null_behaviour</code>: should be one of:</p> <ul> <li><code>IGNORE</code>: we just ignore nulls in the column;</li> <li><code>EMPTY_STRING</code>: we should pass empty string instead of NULL;</li> <li><code>FAIL</code>: our analyzer should fail if there is NULL;</li> </ul> <p><code>filtered_row_outcome</code>: should be one of:</p> <ul> <li><code>NULL</code>: in that case filtered rows should be marked as NULL;</li> <li><code>TRUE</code>: in that case filtered rows should be marked as Boolean <code>True</code>;</li> </ul> <p>The default implementation use <code>NullBehaviour.IGNORE</code> and <code>FilteredRowOutcome.NULL</code> values.</p>"},{"location":"python/main_structures/#where-option","title":"Where option","text":"<p>Most analyzers accept <code>where</code> as an optional parameter. It should be an SQL-like predicate that is applied before the computation of the metric and comparison with the constraint. This can be useful, for example, when you need to compute metrics for different groups.</p> <p>Let's imagine we heave the following data:</p> <pre><code>  +------------+-----------+------------+-----------+-------+\n  |sepal_length|sepal_width|petal_length|petal_width|species|\n  +------------+-----------+------------+-----------+-------+\n  |         5.1|        3.5|         1.4|        0.2| setosa|\n  |         4.9|        3.0|         1.4|        0.2| setosa|\n  |         4.7|        3.2|         1.3|        0.2| setosa|\n  |         4.6|        3.1|         1.5|        0.2| setosa|\n  |         5.0|        3.6|         1.4|        0.2| setosa|\n  |         5.4|        3.9|         1.7|        0.4| setosa|\n  |         4.6|        3.4|         1.4|        0.3| setosa|\n  |         5.0|        3.4|         1.5|        0.2| setosa|\n  |         4.4|        2.9|         1.4|        0.2| setosa|\n  |         4.9|        3.1|         1.5|        0.1| setosa|\n  |         5.4|        3.7|         1.5|        0.2| setosa|\n  |         4.8|        3.4|         1.6|        0.2| setosa|\n  |         4.8|        3.0|         1.4|        0.1| setosa|\n  |         4.3|        3.0|         1.1|        0.1| setosa|\n  |         5.8|        4.0|         1.2|        0.2| setosa|\n  |         5.7|        4.4|         1.5|        0.4| setosa|\n  |         5.4|        3.9|         1.3|        0.4| setosa|\n  |         5.1|        3.5|         1.4|        0.3| setosa|\n  |         5.7|        3.8|         1.7|        0.3| setosa|\n  |         5.1|        3.8|         1.5|        0.3| setosa|\n  +------------+-----------+------------+-----------+-------+\n</code></pre> <p>It is a well-know \"Irir Dataset\" (Fisher, Ronald A. \"The use of multiple measurements in taxonomic problems.\" Annals of eugenics 7.2 (1936): 179-188., link). It contains data about different kinds of irises.</p> <p>Our goal will be to perform data profiling on this dataset, not for all rows together, but based on the different species.</p> <pre><code>  from tsumugi.verification import VerificationSuite\n  from tsumugi.analyzers import Mean\n  from tsumugi.checks import CheckBuilder\n\n  suite = (\n      VerificationSuite\n      .on_data(iris)\n      .add_required_analyzer(\n          Mean(column=\"sepal_length\", where=\"species = 'setosa'\")\n      )\n      .add_required_analyzer(\n          Mean(column=\"sepal_length\", where=\"species = 'versicolor'\")\n      )\n  )\n\n  result = suite.run()\n</code></pre> <p>As a a result we will ge the following values for each metric:</p> <pre><code>     entity      instance                                  name  value\n  0  Column  sepal_length      Mean (where: species = 'setosa')  5.006\n  1  Column  sepal_length  Mean (where: species = 'versicolor')  5.936\n  2  Column  sepal_length   Mean (where: species = 'virginica')  6.588\n</code></pre>"},{"location":"python/main_structures/#constraints","title":"Constraints","text":"<p>Constraints in Tsumugi are a combination of an analyzer, assertion, and additional metadata. To create a new constraint object, it is highly recommended to use the <code>ConstraintBuilder</code>, which is a helper class with user-friendly methods. Under the hood, this class will create a new proto class with a constraint.</p> <pre><code>  class ConstraintBuilder:\n      def __init__(self) -&gt; None:\n\n      def for_analyzer(self, analyzer: AbstractAnalyzer) -&gt; Self:\n          \"\"\"Set an analyzer.\"\"\"\n\n      def with_name(self, name: str) -&gt; Self:\n          \"\"\"Set a name of the constraint.\"\"\"\n\n      def with_hint(self, hint: str) -&gt; Self:\n          \"\"\"Set a hint for the constraint.\n\n          Hint can be helpful in the case when one needs\n          to realize the reason of the constraint or why did it fail.\n          \"\"\"\n\n      def should_be_gt_than(self, value) -&gt; Self:\n      def should_be_geq_than(self, value) -&gt; Self:\n      def should_be_eq_to(self, value) -&gt; Self:\n      def should_be_lt_than(self, value) -&gt; Self:\n      def should_be_leq_than(self, value) -&gt; Self:\n\n      def build(self) -&gt; suite.Check.Constraint:\n</code></pre> <p>WARNING: The <code>should_be_*</code> methods add a constraint value and a comparison sign to the proto message. These methods are type-sensitive! They work slightly differently depending on the type passed. For example, passing a <code>float</code> constraint for a <code>Size</code> analyzer that computes a <code>Long</code> metric under the hood will always return <code>False</code> because, for the JVM, <code>1 != 1.0</code>!</p>"},{"location":"python/main_structures/#alternative-definition-of-constraints","title":"Alternative definition of constraints","text":"<p>Alternatively, users can call a syntactic sugar method from the <code>CheckBuilder</code> class, which we will explain in detail later. For example, the <code>hasSize</code> method is simply a syntactic sugar on top of the definition of the <code>Constraint</code> for the <code>Size</code> analyzer, where the metric should be equal to the passed value:</p> <pre><code>  def has_size(\n      self, expected_size: int, hint: str = \"\", name: str | None = None\n  ) -&gt; Self:\n      \"\"\"Add a constraint that the DataFrame has size like expected.\"\"\"\n      return self.with_constraint(\n          ConstraintBuilder()\n          .for_analyzer(Size())\n          .with_hint(hint)\n          .with_name(name or \"Size\")\n          .should_be_eq_to(expected_size)\n          .build()\n      )\n</code></pre> <p>But not all the possible constraints can be created in this way. You may expecte more syntax sugar like this in the future versions.</p>"},{"location":"python/main_structures/#checks","title":"Checks","text":"<p>Although <code>Check</code> is a top-level structure in Deequ, it is not directly exposed in Tsumugi. Instead, it is recommended to use <code>CheckBuilder</code>. This helper class allows for easy generation of the completed <code>proto.suite.Check</code> object, which is actually a protobuf message class.</p>"},{"location":"python/main_structures/#level","title":"Level","text":"<p>Each check has its own severity level. Currently, upstream Deequ only supports <code>Error</code> and <code>Warning</code> levels, with <code>Warning</code> being the default in both Tsumugi and upstream Deequ. You can modify the severity level of a <code>Check</code> in the builder by calling the method <code>with_level(level)</code>, where <code>level</code> is a value from the <code>CheckLevel</code> enumeration.</p>"},{"location":"python/main_structures/#description","title":"Description","text":"<p>Each check can have its own description that will be exported to the results. The description is simply a regular string. You can modify it using the <code>with_description(description)</code> method of the <code>CheckBuilder</code> class.</p>"},{"location":"python/main_structures/#constraints_1","title":"Constraints","text":"<p><code>CheckBuilder</code> provides two generic methods for adding constraints:</p> <ol> <li><code>with_constraint(constraint)</code>: This method expects an already built <code>Constraint</code> and adds it to the list of constraints.</li> <li><code>with_constraints(constraints)</code>: This method expects a collection of built constraints. It's important to note that this method replaces any existing constraints!</li> </ol> <p>Additionally, <code>CheckBuilder</code> provides a lot of syntactic sugar, such as <code>has_size</code> or <code>is_complete</code>. These methods offer a simpler approach to adding constraints. However, the trade-off is that not all possible constraints have a corresponding syntactic sugar method.</p>"},{"location":"python/main_structures/#verificationsuite","title":"VerificationSuite","text":"<p>The <code>VerificationSuite</code> is not directly accessible in Tsumugi. Instead, it is recommended to use the <code>VerificationRunBuilder</code>. You can create a builder either by directly providing a <code>DataFrame</code> or by calling the backward-compatibility helper <code>VerificationSuite.on_date(df)</code>, which mimics the behavior of <code>python-deequ</code>.</p>"},{"location":"python/main_structures/#row-level-results","title":"Row-level results","text":"<p>Row-level results are not returned by default. To request them, simply call the <code>with_row_level_results()</code> method of the builder. Otherwise, the <code>row_level_result</code> attribute of the object returned by Tsumugi will be <code>None</code>.</p>"},{"location":"python/main_structures/#required-analyzers","title":"Required analyzers","text":"<p>There are two methods for adding required analyzers. The first is <code>add_required_analyzers(analyzers)</code>, which replaces all existing required analyzers with values from the provided collection. The second method is <code>add_required_analyzer</code>, which appends an additional analyzer to the existing ones. Both methods expect an <code>AbstractAnalyzer</code> object, so users don't need to manually create protobuf classes from analyzers.</p>"},{"location":"python/main_structures/#checks_1","title":"Checks","text":"<p>The same two methods are provided for adding checks:</p> <ul> <li><code>add_check(check)</code> appends a single check to the suite</li> <li><code>add_checks(checks)</code> replace all the already added checks by the provided collection</li> </ul> <p>When adding a <code>Check</code>, it is expected to be a protobuf class. Therefore, users need to explicitly call <code>CheckBuilder()....build()</code> themselves.</p>"},{"location":"python/main_structures/#anomaly-detection","title":"Anomaly Detection","text":"<p>Similar to required analyzers and checks, two methods are provided: one for adding a single anomaly detection case, and another for replacing all anomaly detection cases with the provided collection:</p> <ul> <li><code>add_anomaly_detection</code></li> <li><code>add_anomaly_detections</code></li> </ul>"},{"location":"python/main_structures/#key-and-repository","title":"Key and repository","text":"<p>By design of the upstream Deequ, any anomaly detection requires both a repository and a dataset key. There are two separate methods for this in the <code>VerificationRunBuilder</code>. One method adds a <code>FileSystemRepository</code> that should be a path to the JSON file, while the other adds a <code>SparkTableRepository</code> that expects a Spark table name.</p> <pre><code>  def with_fs_repository_and_key(\n      self,\n      filepath: str,\n      dataset_date: int,\n      dataset_tags: dict[str, str] | None = None,\n  ) -&gt; Self:\n      \"\"\"Add a FileSystem repository and date and tags for the ResultKey.\"\"\"\n\n  def with_table_repository_and_key(\n      self,\n      table_name: str,\n      dataset_date: int,\n      dateset_tags: dict[str, str] | None = None,\n  ) -&gt; Self:\n      \"\"\"Add a Table repository and date and tags for the ResultKey.\"\"\"\n</code></pre> <p>The <code>dataset_date</code> is the most crucial element here, as it serves as the value Deequ will use to filter the required time series data of metrics for anomaly detection. This value should be a valid Java <code>Long</code> (less than or equal to <code>9,223,372,036,854,775,807</code>). Passing a larger value may lead to unexpected behavior. It is also not recommended to use negative values, as by design and logic, it should represent something similar to a timestamp.</p> <p>The <code>dataset_tags</code> parameter is not used within Deequ itself but is exposed to the output repository. If you plan to use the repository directly (as a table or as JSON), you can include additional information here.</p>"},{"location":"python/main_structures/#anomaly-detection-builder","title":"Anomaly Detection Builder","text":"<p>As described in the concepts section, anomaly detection is a combination of an analyzer, strategy, and additional metadata. To simplify the creation of AD-checks, an <code>AnomalyDetectionBuilder</code> is provided. Calling <code>build()</code> on this builder will generate a complete protobuf class for the defined anomaly detection case.</p> <pre><code>  class AnomalyDetectionBuilder:\n      \"\"\"Helper object to build AnomalyDetection check.\"\"\"\n\n      def for_analyzer(self, analyzer: AbstractAnalyzer) -&gt; Self:\n          \"\"\"Add an analyzer.\"\"\"\n\n      def with_strategy(self, strategy: AbstractStrategy) -&gt; Self:\n          \"\"\"Add a strategy.\"\"\"\n\n      def with_check_level(self, level: CheckLevel) -&gt; Self:\n          \"\"\"Set a severity level.\"\"\"\n\n      def with_description(self, description: str) -&gt; Self:\n          \"\"\"Add a description.\"\"\"\n\n      def with_tags(self, tags: dict[str, str]) -&gt; Self:\n          \"\"\"Add tags.\"\"\"\n\n      def after_date(self, dt: int) -&gt; Self:\n          \"\"\"Set a minimal dataset date value\n\n          This value will be used to filter out part of\n          the timeseries of metrics.\n          \"\"\"\n\n      def before_date(self, dt: int) -&gt; Self:\n          \"\"\"Set a maximal dataset date value\n\n          This value will be used to filter out part of\n          the timeseries of metrics.\n          \"\"\"\n</code></pre> <p>All the strategies are implemented in the same way as analyzers. They are actually just dataclasses with a single inherited private method that converts a dataclass to the corresponding protobuf class.</p> <p>Note: This is not a bug, but a feature of upstream Deequ. If you run your anomaly detection case with the same <code>dataset_tag</code> twice in a row, the first run will be compared to a previous run, but the second run will be compared with the first run. To avoid this behavior, add any value less than your <code>dataset_tag</code> to the <code>before_date</code> parameter in the builder.</p>"},{"location":"python/reference/SUMMARY/","title":"API Reference","text":"<ul> <li>tsumugi-python<ul> <li>tsumugi<ul> <li>analyzers</li> <li>anomaly_detection</li> <li>checks</li> <li>enums</li> <li>repository</li> <li>utils</li> <li>verification</li> </ul> </li> </ul> </li> </ul>"},{"location":"python/reference/tsumugi-python/tsumugi/","title":"Index","text":""},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/","title":"Analyzers","text":""},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.AbstractAggregateFunction","title":"<code>AbstractAggregateFunction</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class Histogram aggregation functions.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>class AbstractAggregateFunction(ABC):\n    \"\"\"Abstract class Histogram aggregation functions.\"\"\"\n\n    @abstractmethod\n    def _to_proto(self) -&gt; proto.Histogram.AggregateFunction: ...\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.AbstractAnalyzer","title":"<code>AbstractAnalyzer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for all analyzers in tsumugi.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>class AbstractAnalyzer(ABC):\n    \"\"\"Abstract class for all analyzers in tsumugi.\"\"\"\n\n    @abstractmethod\n    def _to_proto(self) -&gt; proto.Analyzer: ...\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.AnalyzerOptions","title":"<code>AnalyzerOptions</code>  <code>dataclass</code>","text":"<p>Container for Analyzer Options.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass AnalyzerOptions:\n    \"\"\"Container for Analyzer Options.\"\"\"\n\n    null_behaviour: NullBehaviour\n    filtered_row_outcome: FilteredRowOutcome\n\n    @staticmethod\n    def default() -&gt; \"AnalyzerOptions\":\n        return AnalyzerOptions(NullBehaviour.IGNORE, FilteredRowOutcome.NULL)\n\n    def _to_proto(self) -&gt; proto.AnalyzerOptions:\n        return proto.AnalyzerOptions(\n            null_behaviour=self.null_behaviour.value,\n            filtered_row_outcome=self.filtered_row_outcome.value,\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ApproxCountDistinct","title":"<code>ApproxCountDistinct</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Computes the approximate count distinctness of a column with HyperLogLogPlusPlus.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass ApproxCountDistinct(AbstractAnalyzer):\n    \"\"\"Computes the approximate count distinctness of a column with HyperLogLogPlusPlus.\"\"\"\n\n    column: str\n    where: str | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            approx_count_distinct=proto.ApproxCountDistinct(\n                column=self.column, where=self.where\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ApproxQuantile","title":"<code>ApproxQuantile</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Computes the Approximate Quantile of a column.</p> <p>The allowed relative error compared to the exact quantile can be configured with the <code>relativeError</code> parameter.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass ApproxQuantile(AbstractAnalyzer):\n    \"\"\"Computes the Approximate Quantile of a column.\n\n    The allowed relative error compared to the exact quantile can be configured with the\n    `relativeError` parameter.\n    \"\"\"\n\n    column: str\n    quantile: float\n    relative_error: float | None = None\n    where: str | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            approx_quantile=proto.ApproxQuantile(\n                column=self.column,\n                quantile=self.quantile,\n                relative_error=self.relative_error,\n                where=self.where,\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ApproxQuantiles","title":"<code>ApproxQuantiles</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Computes the approximate quantiles of a column.</p> <p>The allowed relative error compared to the exact quantile can be configured with <code>relativeError</code> parameter.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass ApproxQuantiles(AbstractAnalyzer):\n    \"\"\"Computes the approximate quantiles of a column.\n\n    The allowed relative error compared to the exact quantile can be configured with\n    `relativeError` parameter.\n    \"\"\"\n\n    column: str\n    quantiles: list[float]\n    relative_error: float | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            approx_quantiles=proto.ApproxQuantiles(\n                column=self.column,\n                quantiles=self.quantiles,\n                relative_error=self.relative_error,\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ColumnCount","title":"<code>ColumnCount</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Computes the count of columns.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass ColumnCount(AbstractAnalyzer):\n    \"\"\"Computes the count of columns.\"\"\"\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(column_count=proto.ColumnCount())\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.Completeness","title":"<code>Completeness</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Completeness is the fraction of non-null values in a column.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass Completeness(AbstractAnalyzer):\n    \"\"\"Completeness is the fraction of non-null values in a column.\"\"\"\n\n    column: str\n    where: str | None = None\n    options: AnalyzerOptions = AnalyzerOptions.default()\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            completeness=proto.Completeness(\n                column=self.column, where=self.where, options=self.options._to_proto()\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.Compliance","title":"<code>Compliance</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Compliance measures the fraction of rows that complies with the given column constraint.</p> <p>E.g if the constraint is \"att1&gt;3\" and data frame has 5 rows with att1 column value greater than 3 and 10 rows under 3; a DoubleMetric would be returned with 0.33 value.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass Compliance(AbstractAnalyzer):\n    \"\"\"Compliance measures the fraction of rows that complies with the given column constraint.\n\n    E.g if the constraint is \"att1&gt;3\" and data frame has 5 rows with att1 column value greater\n    than 3 and 10 rows under 3; a DoubleMetric would be returned with 0.33 value.\n    \"\"\"\n\n    instance: str\n    predicate: str\n    where: str | None = None\n    columns: list[str] = field(default_factory=list)\n    options: AnalyzerOptions = AnalyzerOptions.default()\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            compliance=proto.Compliance(\n                instance=self.instance,\n                predicate=self.predicate,\n                where=self.where,\n                columns=self.columns,\n                options=self.options._to_proto(),\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ConstraintBuilder","title":"<code>ConstraintBuilder</code>","text":"Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>class ConstraintBuilder:\n    def __init__(self) -&gt; None:\n        self._analyzer: AbstractAnalyzer | None = None\n        self._is_long: bool = False\n        self._expected_value: float | int | None = None\n        self._sign: suite.Check.ComparisonSign | None = None\n        self._hint: str | None = None\n        self._name: str | None = None\n\n    def for_analyzer(self, analyzer: AbstractAnalyzer) -&gt; Self:\n        \"\"\"Set an analyzer.\"\"\"\n\n        self._analyzer = analyzer\n        return self\n\n    def with_name(self, name: str) -&gt; Self:\n        \"\"\"Set a name of the constraint.\"\"\"\n\n        self._name = name\n        return self\n\n    def with_hint(self, hint: str) -&gt; Self:\n        \"\"\"Set a hint for the constraint.\n\n        Hint can be helpful in the case when one needs\n        to realize the reason of the constraint or why did it fail.\n        \"\"\"\n\n        self._hint = hint\n        return self\n\n    @singledispatchmethod\n    def should_be_gt_than(self, value) -&gt; Self:\n        \"\"\"Add an assertion that metric &gt; value.\n\n        This result of this methods depends of the passed type!\n        \"\"\"\n        ...\n\n    @should_be_gt_than.register\n    def _(self, value: int) -&gt; Self:\n        self._sign = suite.Check.ComparisonSign.GT\n        self._is_long = True\n        self._expected_value = value\n        return self\n\n    @should_be_gt_than.register\n    def _(self, value: float) -&gt; Self:\n        self._sign = suite.Check.ComparisonSign.GT\n        self._is_long = False\n        self._expected_value = value\n        return self\n\n    @singledispatchmethod\n    def should_be_geq_than(self, value) -&gt; Self:\n        \"\"\"Add an assertion that metric &gt;= value.\n\n        This result of this methods depends of the passed type!\n        \"\"\"\n        ...\n\n    @should_be_geq_than.register\n    def _(self, value: int) -&gt; Self:\n        self._sign = suite.Check.ComparisonSign.GET\n        self._is_long = True\n        self._expected_value = value\n        return self\n\n    @should_be_geq_than.register\n    def _(self, value: float) -&gt; Self:\n        self._sign = suite.Check.ComparisonSign.GET\n        self._is_long = False\n        self._expected_value = value\n        return self\n\n    @singledispatchmethod\n    def should_be_eq_to(self, value) -&gt; Self:\n        \"\"\"Add an assertion that metric == value.\n\n        This result of this methods depends of the passed type!\n        \"\"\"\n        ...\n\n    @should_be_eq_to.register\n    def _(self, value: int) -&gt; Self:\n        self._sign = suite.Check.ComparisonSign.EQ\n        self._is_long = True\n        self._expected_value = value\n        return self\n\n    @should_be_eq_to.register\n    def _(self, value: float) -&gt; Self:\n        self._sign = suite.Check.ComparisonSign.EQ\n        self._is_long = False\n        self._expected_value = value\n        return self\n\n    @singledispatchmethod\n    def should_be_lt_than(self, value) -&gt; Self:\n        \"\"\"Add an assertion that metric &lt; value.\n\n        This result of this methods depends of the passed type!\n        \"\"\"\n        ...\n\n    @should_be_lt_than.register\n    def _(self, value: int) -&gt; Self:\n        self._sign = suite.Check.ComparisonSign.LT\n        self._is_long = True\n        self._expected_value = value\n        return self\n\n    @should_be_lt_than.register\n    def _(self, value: float) -&gt; Self:\n        self._sign = suite.Check.ComparisonSign.LT\n        self._is_long = False\n        self._expected_value = value\n        return self\n\n    @singledispatchmethod\n    def should_be_leq_than(self, value) -&gt; Self:\n        \"\"\"Add an assertion that metric &lt;= value.\n\n        This result of this methods depends of the passed type!\n        \"\"\"\n        ...\n\n    @should_be_leq_than.register\n    def _(self, value: int) -&gt; Self:\n        self._sign = suite.Check.ComparisonSign.LET\n        self._is_long = True\n        self._expected_value = value\n        return self\n\n    @should_be_leq_than.register\n    def _(self, value: float) -&gt; Self:\n        self._sign = suite.Check.ComparisonSign.LET\n        self._is_long = False\n        self._expected_value = value\n        return self\n\n    def _validate(self) -&gt; None:\n        if self._analyzer is None:\n            raise ValueError(\"Analyzer is not set\")\n        if self._expected_value is None:\n            raise ValueError(\"Expected value is not set\")\n\n    def build(self) -&gt; suite.Check.Constraint:\n        self._validate()\n\n        # for mypy\n        assert self._analyzer is not None\n        assert self._expected_value is not None\n\n        if self._is_long:\n            # for mypy\n            assert isinstance(self._expected_value, int)\n\n            return suite.Check.Constraint(\n                analyzer=self._analyzer._to_proto(),\n                long_expectation=self._expected_value,\n                sign=self._sign,\n                hint=self._hint,\n                name=self._name,\n            )\n        else:\n            # for mypy\n            assert isinstance(self._expected_value, float)\n\n            return suite.Check.Constraint(\n                analyzer=self._analyzer._to_proto(),\n                double_expectation=self._expected_value,\n                sign=self._sign,\n                hint=self._hint,\n                name=self._name,\n            )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ConstraintBuilder.for_analyzer","title":"<code>for_analyzer(analyzer)</code>","text":"<p>Set an analyzer.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>def for_analyzer(self, analyzer: AbstractAnalyzer) -&gt; Self:\n    \"\"\"Set an analyzer.\"\"\"\n\n    self._analyzer = analyzer\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ConstraintBuilder.should_be_eq_to","title":"<code>should_be_eq_to(value)</code>","text":"<p>Add an assertion that metric == value.</p> <p>This result of this methods depends of the passed type!</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@singledispatchmethod\ndef should_be_eq_to(self, value) -&gt; Self:\n    \"\"\"Add an assertion that metric == value.\n\n    This result of this methods depends of the passed type!\n    \"\"\"\n    ...\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ConstraintBuilder.should_be_geq_than","title":"<code>should_be_geq_than(value)</code>","text":"<p>Add an assertion that metric &gt;= value.</p> <p>This result of this methods depends of the passed type!</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@singledispatchmethod\ndef should_be_geq_than(self, value) -&gt; Self:\n    \"\"\"Add an assertion that metric &gt;= value.\n\n    This result of this methods depends of the passed type!\n    \"\"\"\n    ...\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ConstraintBuilder.should_be_gt_than","title":"<code>should_be_gt_than(value)</code>","text":"<p>Add an assertion that metric &gt; value.</p> <p>This result of this methods depends of the passed type!</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@singledispatchmethod\ndef should_be_gt_than(self, value) -&gt; Self:\n    \"\"\"Add an assertion that metric &gt; value.\n\n    This result of this methods depends of the passed type!\n    \"\"\"\n    ...\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ConstraintBuilder.should_be_leq_than","title":"<code>should_be_leq_than(value)</code>","text":"<p>Add an assertion that metric &lt;= value.</p> <p>This result of this methods depends of the passed type!</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@singledispatchmethod\ndef should_be_leq_than(self, value) -&gt; Self:\n    \"\"\"Add an assertion that metric &lt;= value.\n\n    This result of this methods depends of the passed type!\n    \"\"\"\n    ...\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ConstraintBuilder.should_be_lt_than","title":"<code>should_be_lt_than(value)</code>","text":"<p>Add an assertion that metric &lt; value.</p> <p>This result of this methods depends of the passed type!</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@singledispatchmethod\ndef should_be_lt_than(self, value) -&gt; Self:\n    \"\"\"Add an assertion that metric &lt; value.\n\n    This result of this methods depends of the passed type!\n    \"\"\"\n    ...\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ConstraintBuilder.with_hint","title":"<code>with_hint(hint)</code>","text":"<p>Set a hint for the constraint.</p> <p>Hint can be helpful in the case when one needs to realize the reason of the constraint or why did it fail.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>def with_hint(self, hint: str) -&gt; Self:\n    \"\"\"Set a hint for the constraint.\n\n    Hint can be helpful in the case when one needs\n    to realize the reason of the constraint or why did it fail.\n    \"\"\"\n\n    self._hint = hint\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ConstraintBuilder.with_name","title":"<code>with_name(name)</code>","text":"<p>Set a name of the constraint.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>def with_name(self, name: str) -&gt; Self:\n    \"\"\"Set a name of the constraint.\"\"\"\n\n    self._name = name\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.Correlation","title":"<code>Correlation</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Computes the pearson correlation coefficient between the two given columns.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass Correlation(AbstractAnalyzer):\n    \"\"\"Computes the pearson correlation coefficient between the two given columns.\"\"\"\n\n    first_column: str\n    second_column: str\n    where: str | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            correlation=proto.Correlation(\n                first_column=self.first_column,\n                second_column=self.second_column,\n                where=self.where,\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.CountAggregate","title":"<code>CountAggregate</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAggregateFunction</code></p> <p>Computes Histogram Count Aggregation</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass CountAggregate(AbstractAggregateFunction):\n    \"\"\"Computes Histogram Count Aggregation\"\"\"\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Histogram.AggregateFunction.Count()\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.CountDistinct","title":"<code>CountDistinct</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Counts the distinct elements in the column(s).</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass CountDistinct(AbstractAnalyzer):\n    \"\"\"Counts the distinct elements in the column(s).\"\"\"\n\n    columns: list[str] = field(default_factory=list)\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(count_distinct=proto.CountDistinct(columns=self.columns))\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.CustomSql","title":"<code>CustomSql</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Compute the number of rows that match the custom SQL expression.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass CustomSql(AbstractAnalyzer):\n    \"\"\"Compute the number of rows that match the custom SQL expression.\"\"\"\n\n    expressions: str\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(custom_sql=proto.CustomSql(expressions=self.expressions))\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.DataType","title":"<code>DataType</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Data Type Analyzer. Returns the datatypes of column.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass DataType(AbstractAnalyzer):\n    \"\"\"Data Type Analyzer. Returns the datatypes of column.\"\"\"\n\n    column: str\n    where: str | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            data_type=proto.DataType(column=self.column, where=self.where)\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.Distinctness","title":"<code>Distinctness</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Count the distinctness of elements in column(s).</p> <p>Distinctness is the fraction of distinct values of a column(s).</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass Distinctness(AbstractAnalyzer):\n    \"\"\"Count the distinctness of elements in column(s).\n\n    Distinctness is the fraction of distinct values of a column(s).\n    \"\"\"\n\n    columns: list[str] = field(default_factory=list)\n    where: str | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            distinctness=proto.Distinctness(columns=self.columns, where=self.where)\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.Entropy","title":"<code>Entropy</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Entropy is a measure of the level of information contained in a message.</p> <p>Given the probability distribution over values in a column, it describes how many bits are required to identify a value.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass Entropy(AbstractAnalyzer):\n    \"\"\"Entropy is a measure of the level of information contained in a message.\n\n    Given the probability distribution over values in a column, it describes\n    how many bits are required to identify a value.\n    \"\"\"\n\n    column: str\n    where: str | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            entropy=proto.Entropy(column=self.column, where=self.where)\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.ExactQuantile","title":"<code>ExactQuantile</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Compute an exact quantile of the given column.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass ExactQuantile(AbstractAnalyzer):\n    \"\"\"Compute an exact quantile of the given column.\"\"\"\n\n    column: str\n    quantile: float\n    where: str | None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            exact_quantile=proto.ExactQuantile(\n                column=self.column, quantile=self.quantile, where=self.where\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.Histogram","title":"<code>Histogram</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Histogram is the summary of values in a column of a DataFrame.</p> <p>It groups the column's values then calculates the number of rows with that specific value and the fraction of the value.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass Histogram(AbstractAnalyzer):\n    \"\"\"Histogram is the summary of values in a column of a DataFrame.\n\n    It groups the column's values then calculates the number of rows with\n    that specific value and the fraction of the value.\n    \"\"\"\n\n    column: str\n    max_detail_bin: int | None = None\n    where: str | None = None\n    compute_frequencies_as_ratio: bool = True\n    aggregate_function: AbstractAggregateFunction = CountAggregate()\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            histogram=proto.Histogram(\n                column=self.column,\n                max_detail_bins=self.max_detail_bin,\n                where=self.where,\n                compute_frequencies_as_ratio=self.compute_frequencies_as_ratio,\n                aggregate_function=self.aggregate_function._to_proto(),\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.KLLParameters","title":"<code>KLLParameters</code>  <code>dataclass</code>","text":"<p>Parameters for KLLSketch.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass KLLParameters:\n    \"\"\"Parameters for KLLSketch.\"\"\"\n\n    sketch_size: int\n    shrinking_factor: float\n    number_of_buckets: int\n\n    def _to_proto(self) -&gt; proto.KLLSketch.KLLParameters:\n        return proto.KLLSketch.KLLParameters(\n            sketch_size=self.sketch_size,\n            shrinking_factor=self.shrinking_factor,\n            number_of_buckets=self.number_of_buckets,\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.KLLSketch","title":"<code>KLLSketch</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>The KLL Sketch analyzer.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass KLLSketch(AbstractAnalyzer):\n    \"\"\"The KLL Sketch analyzer.\"\"\"\n\n    column: str\n    kll_parameters: KLLParameters | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            kll_sketch=proto.KLLSketch(\n                column=self.column,\n                kll_parameters=(\n                    self.kll_parameters._to_proto() if self.kll_parameters else None\n                ),\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.MaxLength","title":"<code>MaxLength</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>MaxLength Analyzer. Get Max length of a str type column.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass MaxLength(AbstractAnalyzer):\n    \"\"\"MaxLength Analyzer. Get Max length of a str type column.\"\"\"\n\n    column: str\n    where: str | None = None\n    options: AnalyzerOptions = AnalyzerOptions.default()\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            max_length=proto.MaxLength(\n                column=self.column,\n                where=self.where,\n                options=self.options._to_proto(),\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.Maximum","title":"<code>Maximum</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Get the maximum of a numeric column.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass Maximum(AbstractAnalyzer):\n    \"\"\"Get the maximum of a numeric column.\"\"\"\n\n    column: str\n    where: str | None = None\n    options: AnalyzerOptions = AnalyzerOptions.default()\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            maximum=proto.Maximum(\n                column=self.column,\n                where=self.where,\n                options=self.options._to_proto(),\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.Mean","title":"<code>Mean</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Mean Analyzer. Get mean of a column.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass Mean(AbstractAnalyzer):\n    \"\"\"Mean Analyzer. Get mean of a column.\"\"\"\n\n    column: str\n    where: str | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            mean=proto.Mean(\n                column=self.column,\n                where=self.where,\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.MinLength","title":"<code>MinLength</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Get the minimum length of a column.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass MinLength(AbstractAnalyzer):\n    \"\"\"Get the minimum length of a column.\"\"\"\n\n    column: str\n    where: str | None = None\n    options: AnalyzerOptions = AnalyzerOptions.default()\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            min_length=proto.MinLength(\n                column=self.column,\n                where=self.where,\n                options=self.options._to_proto(),\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.Minimum","title":"<code>Minimum</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Get the minimum of a numeric column.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass Minimum(AbstractAnalyzer):\n    \"\"\"Get the minimum of a numeric column.\"\"\"\n\n    column: str\n    where: str | None = None\n    options: AnalyzerOptions = AnalyzerOptions.default()\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            minimum=proto.Minimum(\n                column=self.column,\n                where=self.where,\n                options=self.options._to_proto(),\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.MutualInformation","title":"<code>MutualInformation</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Describes how much information about one column can be inferred from another column.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass MutualInformation(AbstractAnalyzer):\n    \"\"\"Describes how much information about one column can be inferred from another column.\"\"\"\n\n    columns: list[str] = field(default_factory=list)\n    where: str | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            mutual_information=proto.MutualInformation(\n                columns=self.columns,\n                where=self.where,\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.PatternMatch","title":"<code>PatternMatch</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>PatternMatch is a measure of the fraction of rows that complies with a given column regex constraint.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass PatternMatch(AbstractAnalyzer):\n    \"\"\"PatternMatch is a measure of the fraction of rows that complies with a\n    given column regex constraint.\"\"\"\n\n    column: str\n    pattern: str\n    where: str | None = None\n    options: AnalyzerOptions = AnalyzerOptions.default()\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            pattern_match=proto.PatternMatch(\n                column=self.column,\n                pattern=self.pattern,\n                where=self.where,\n                options=self.options._to_proto(),\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.RatioOfSums","title":"<code>RatioOfSums</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Compute ratio of sums between two columns.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass RatioOfSums(AbstractAnalyzer):\n    \"\"\"Compute ratio of sums between two columns.\"\"\"\n\n    numerator: str\n    denominator: str\n    where: str | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            ratio_of_sums=proto.RatioOfSums(\n                numerator=self.numerator,\n                denominator=self.denominator,\n                where=self.where,\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.Size","title":"<code>Size</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Size is the number of rows in a DataFrame.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass Size(AbstractAnalyzer):\n    \"\"\"Size is the number of rows in a DataFrame.\"\"\"\n\n    where: str | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(size=proto.Size(where=self.where))\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.StandardDeviation","title":"<code>StandardDeviation</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Calculates the Standard Deviation of column.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass StandardDeviation(AbstractAnalyzer):\n    \"\"\"Calculates the Standard Deviation of column.\"\"\"\n\n    column: str\n    where: str | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            standard_deviation=proto.StandardDeviation(\n                column=self.column,\n                where=self.where,\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.Sum","title":"<code>Sum</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Calculates the sum of a column.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass Sum(AbstractAnalyzer):\n    \"\"\"Calculates the sum of a column.\"\"\"\n\n    column: str\n    where: str | None = None\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            sum=proto.Sum(\n                column=self.column,\n                where=self.where,\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.SumAggregate","title":"<code>SumAggregate</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAggregateFunction</code></p> <p>Computes Histogram Sum Aggregation</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass SumAggregate(AbstractAggregateFunction):\n    \"\"\"Computes Histogram Sum Aggregation\"\"\"\n\n    agg_column: str\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Histogram.AggregateFunction.Sum(agg_column=self.agg_column)\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.UniqueValueRatio","title":"<code>UniqueValueRatio</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Compute the ratio of uniqu values for columns.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass UniqueValueRatio(AbstractAnalyzer):\n    \"\"\"Compute the ratio of uniqu values for columns.\"\"\"\n\n    columns: list[str] = field(default_factory=list)\n    where: str | None = None\n    options: AnalyzerOptions = AnalyzerOptions.default()\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            unique_value_ratio=proto.UniqueValueRatio(\n                columns=self.columns,\n                where=self.where,\n                options=self.options._to_proto(),\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/analyzers/#tsumugi-python.tsumugi.analyzers.Uniqueness","title":"<code>Uniqueness</code>  <code>dataclass</code>","text":"<p>               Bases: <code>AbstractAnalyzer</code></p> <p>Compute the uniqueness of the columns.</p> Source code in <code>tsumugi-python/tsumugi/analyzers.py</code> <pre><code>@dataclass\nclass Uniqueness(AbstractAnalyzer):\n    \"\"\"Compute the uniqueness of the columns.\"\"\"\n\n    columns: list[str] = field(default_factory=list)\n    where: str | None = None\n    options: AnalyzerOptions = AnalyzerOptions.default()\n\n    def _to_proto(self) -&gt; proto.Analyzer:\n        return proto.Analyzer(\n            uniqueness=proto.Uniqueness(\n                columns=self.columns,\n                where=self.where,\n                options=self.options._to_proto(),\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/anomaly_detection/","title":"Anomaly detection","text":""},{"location":"python/reference/tsumugi-python/tsumugi/anomaly_detection/#tsumugi-python.tsumugi.anomaly_detection.AnomalyDetectionBuilder","title":"<code>AnomalyDetectionBuilder</code>","text":"<p>Helper object to build AnomalyDetection check.</p> Source code in <code>tsumugi-python/tsumugi/anomaly_detection.py</code> <pre><code>class AnomalyDetectionBuilder:\n    \"\"\"Helper object to build AnomalyDetection check.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._strategy: AbstractStrategy | None = None\n        self._analyzer: AbstractAnalyzer | None = None\n        self._check_level: CheckLevel = CheckLevel.Warning\n        self._description: str = \"\"\n        self._with_tag_values: dict[str, str] = dict()\n        self._after_date: int | None = None\n        self._before_date: int | None = None\n\n    def for_analyzer(self, analyzer: AbstractAnalyzer) -&gt; Self:\n        \"\"\"Add an analyzer.\"\"\"\n\n        self._analyzer = analyzer\n        return self\n\n    def with_strategy(self, strategy: AbstractStrategy) -&gt; Self:\n        \"\"\"Add a strategy.\"\"\"\n\n        self._strategy = strategy\n        return self\n\n    def with_check_level(self, level: CheckLevel) -&gt; Self:\n        \"\"\"Set a severity level.\"\"\"\n\n        self._check_level = level\n        return self\n\n    def with_description(self, description: str) -&gt; Self:\n        \"\"\"Add a description.\"\"\"\n\n        self._description = description\n        return self\n\n    def with_tags(self, tags: dict[str, str]) -&gt; Self:\n        \"\"\"Add tags.\"\"\"\n\n        self._with_tag_values = tags\n        return self\n\n    def after_date(self, dt: int) -&gt; Self:\n        \"\"\"Set a minimal dataset date value\n\n        This value will be used to filter out part of\n        the timeseries of metrics.\n        \"\"\"\n        self._after_date = dt\n        return self\n\n    def before_date(self, dt: int) -&gt; Self:\n        \"\"\"Set a maximal dataset date value\n\n        This value will be used to filter out part of\n        the timeseries of metrics.\n        \"\"\"\n\n        self._before_date = dt\n        return self\n\n    def _validate(self) -&gt; None:\n        if self._analyzer is None:\n            raise ValueError(\"Analyzer is not set\")\n        if self._strategy is None:\n            raise ValueError(\"Strategy is not set\")\n\n    def build(self) -&gt; suite.AnomalyDetection:\n        self._validate()\n\n        # for mypy\n        assert self._strategy is not None\n        assert self._analyzer is not None\n\n        return suite.AnomalyDetection(\n            anomaly_detection_strategy=self._strategy._to_proto(),\n            analyzer=self._analyzer._to_proto(),\n            config=suite.AnomalyDetection.AnomalyCheckConfig(\n                level=self._check_level._to_proto(),\n                description=self._description,\n                with_tag_values=self._with_tag_values,\n                after_date=self._after_date,\n                before_date=self._before_date,\n            ),\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/anomaly_detection/#tsumugi-python.tsumugi.anomaly_detection.AnomalyDetectionBuilder.after_date","title":"<code>after_date(dt)</code>","text":"<p>Set a minimal dataset date value</p> <p>This value will be used to filter out part of the timeseries of metrics.</p> Source code in <code>tsumugi-python/tsumugi/anomaly_detection.py</code> <pre><code>def after_date(self, dt: int) -&gt; Self:\n    \"\"\"Set a minimal dataset date value\n\n    This value will be used to filter out part of\n    the timeseries of metrics.\n    \"\"\"\n    self._after_date = dt\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/anomaly_detection/#tsumugi-python.tsumugi.anomaly_detection.AnomalyDetectionBuilder.before_date","title":"<code>before_date(dt)</code>","text":"<p>Set a maximal dataset date value</p> <p>This value will be used to filter out part of the timeseries of metrics.</p> Source code in <code>tsumugi-python/tsumugi/anomaly_detection.py</code> <pre><code>def before_date(self, dt: int) -&gt; Self:\n    \"\"\"Set a maximal dataset date value\n\n    This value will be used to filter out part of\n    the timeseries of metrics.\n    \"\"\"\n\n    self._before_date = dt\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/anomaly_detection/#tsumugi-python.tsumugi.anomaly_detection.AnomalyDetectionBuilder.for_analyzer","title":"<code>for_analyzer(analyzer)</code>","text":"<p>Add an analyzer.</p> Source code in <code>tsumugi-python/tsumugi/anomaly_detection.py</code> <pre><code>def for_analyzer(self, analyzer: AbstractAnalyzer) -&gt; Self:\n    \"\"\"Add an analyzer.\"\"\"\n\n    self._analyzer = analyzer\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/anomaly_detection/#tsumugi-python.tsumugi.anomaly_detection.AnomalyDetectionBuilder.with_check_level","title":"<code>with_check_level(level)</code>","text":"<p>Set a severity level.</p> Source code in <code>tsumugi-python/tsumugi/anomaly_detection.py</code> <pre><code>def with_check_level(self, level: CheckLevel) -&gt; Self:\n    \"\"\"Set a severity level.\"\"\"\n\n    self._check_level = level\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/anomaly_detection/#tsumugi-python.tsumugi.anomaly_detection.AnomalyDetectionBuilder.with_description","title":"<code>with_description(description)</code>","text":"<p>Add a description.</p> Source code in <code>tsumugi-python/tsumugi/anomaly_detection.py</code> <pre><code>def with_description(self, description: str) -&gt; Self:\n    \"\"\"Add a description.\"\"\"\n\n    self._description = description\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/anomaly_detection/#tsumugi-python.tsumugi.anomaly_detection.AnomalyDetectionBuilder.with_strategy","title":"<code>with_strategy(strategy)</code>","text":"<p>Add a strategy.</p> Source code in <code>tsumugi-python/tsumugi/anomaly_detection.py</code> <pre><code>def with_strategy(self, strategy: AbstractStrategy) -&gt; Self:\n    \"\"\"Add a strategy.\"\"\"\n\n    self._strategy = strategy\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/anomaly_detection/#tsumugi-python.tsumugi.anomaly_detection.AnomalyDetectionBuilder.with_tags","title":"<code>with_tags(tags)</code>","text":"<p>Add tags.</p> Source code in <code>tsumugi-python/tsumugi/anomaly_detection.py</code> <pre><code>def with_tags(self, tags: dict[str, str]) -&gt; Self:\n    \"\"\"Add tags.\"\"\"\n\n    self._with_tag_values = tags\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/","title":"Checks","text":""},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder","title":"<code>CheckBuilder</code>","text":"<p>A helper object to create a constraint.</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>class CheckBuilder:\n    \"\"\"A helper object to create a constraint.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._level: CheckLevel = CheckLevel.Warning\n        self._constraints: list[suite.Check.Constraint] = list()\n        self._description: str = \"\"\n\n    def with_level(self, level: CheckLevel) -&gt; Self:\n        \"\"\"Set a level of the Check.\"\"\"\n        self._level = level\n        return self\n\n    def with_description(self, decription: str) -&gt; Self:\n        \"\"\"Set a description of the Check.\"\"\"\n        self._description = decription\n        return self\n\n    def with_constraint(self, constraint: suite.Check.Constraint) -&gt; Self:\n        \"\"\"Add a constraint to the Check.\n\n        It is recommended to use ConstraintBuilder!\n        \"\"\"\n        self._constraints.append(constraint)\n        return self\n\n    def with_constraints(self, constraints_list: list[suite.Check.Constraint]) -&gt; Self:\n        \"\"\"Set constraints. Override existing!\"\"\"\n        self._constraints = constraints_list\n        return self\n\n    def has_size(\n        self, expected_size: int, hint: str = \"\", name: str | None = None\n    ) -&gt; Self:\n        \"\"\"Add a constraint that the DataFrame has size like expected.\"\"\"\n        return self.with_constraint(\n            ConstraintBuilder()\n            .for_analyzer(Size())\n            .with_hint(hint)\n            .with_name(name or \"Size\")\n            .should_be_eq_to(expected_size)\n            .build()\n        )\n\n    def is_complete(\n        self,\n        column: str,\n        hint: str = \"\",\n        name: str | None = None,\n        where: str | None = None,\n        options: AnalyzerOptions | None = None,\n    ) -&gt; Self:\n        \"\"\"Add a constraint that the column doesn not have missings.\"\"\"\n        return self.has_completeness(column, 1.0, hint, name, where, options)\n\n    def has_completeness(\n        self,\n        column: str,\n        expected_value: float,\n        hint: str = \"\",\n        name: str | None = None,\n        where: str | None = None,\n        options: AnalyzerOptions | None = None,\n    ) -&gt; Self:\n        \"\"\"Add a constraint the the column has an expected part of missings.\"\"\"\n        return self.with_constraint(\n            ConstraintBuilder()\n            .for_analyzer(\n                Completeness(\n                    column=column,\n                    where=where,\n                    options=options or AnalyzerOptions.default(),\n                )\n            )\n            .with_hint(hint)\n            .with_name(name or f\"Completeness({column})\")\n            .should_be_eq_to(expected_value)\n            .build()\n        )\n\n    def are_complete(\n        self,\n        columns: list[str],\n        hint: str = \"\",\n        name: str | None = None,\n        where: str | None = None,\n        options: AnalyzerOptions | None = None,\n    ) -&gt; Self:\n        \"\"\"Add a constraint that columns have an expected part of missings.\"\"\"\n        return self.have_completeness(columns, 1.0, hint, name, where, options)\n\n    def have_completeness(\n        self,\n        columns: list[str],\n        expected_value: float,\n        hint: str = \"\",\n        name: str | None = None,\n        where: str | None = None,\n        options: AnalyzerOptions | None = None,\n    ) -&gt; Self:\n        \"\"\"Add a constraint that columns have an expected level of completeness.\"\"\"\n        list_of_constraints = [\n            ConstraintBuilder()\n            .for_analyzer(\n                Completeness(\n                    column=col,\n                    where=where,\n                    options=options or AnalyzerOptions.default(),\n                )\n            )\n            .with_name(name or f\"Completeness({col})\")\n            .with_hint(hint)\n            .should_be_eq_to(expected_value)\n            .build()\n            for col in columns\n        ]\n        for constraint in list_of_constraints:\n            self = self.with_constraint(constraint=constraint)\n\n        return self\n\n    def is_unique(\n        self,\n        column: str,\n        hint: str = \"\",\n        name: str | None = None,\n        where: str | None = None,\n        options: AnalyzerOptions | None = None,\n    ) -&gt; Self:\n        \"\"\"Create a constraint that the column is unique.\"\"\"\n        return self.has_uniqueness([column], 1.0, hint, name, where, options)\n\n    def is_primary_key(\n        self,\n        column: str,\n        hint: str = \"\",\n        name: str | None = None,\n        where: str | None = None,\n        options: AnalyzerOptions | None = None,\n    ) -&gt; Self:\n        \"\"\"Create a contraint that the column is like a primary key.\"\"\"\n        return self.is_unique(column, hint, name, where, options).is_complete(\n            column, hint, name, where, options\n        )\n\n    def has_uniqueness(\n        self,\n        columns: list[str],\n        expected_value: float,\n        hint: str = \"\",\n        name: str | None = None,\n        where: str | None = None,\n        options: AnalyzerOptions | None = None,\n    ) -&gt; Self:\n        \"\"\"Create a constraint that the given set of columns have an expected level of uniqueness.\"\"\"\n        return self.with_constraint(\n            ConstraintBuilder()\n            .for_analyzer(\n                Uniqueness(\n                    columns=columns,\n                    where=where,\n                    options=options or AnalyzerOptions.default(),\n                )\n            )\n            .with_name(name or f\"Uniquesness{','.join(columns)}\")\n            .with_hint(hint)\n            .should_be_eq_to(expected_value)\n            .build()\n        )\n\n    def _validate(self) -&gt; None:\n        if len(self._constraints) == 0:\n            raise ValueError(\"At least one constraint is required\")\n\n    def build(self) -&gt; suite.Check:\n        \"\"\"Build a Check to the protobuf message.\"\"\"\n        return suite.Check(\n            checkLevel=self._level,\n            description=self._description,\n            constraints=self._constraints,\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.are_complete","title":"<code>are_complete(columns, hint='', name=None, where=None, options=None)</code>","text":"<p>Add a constraint that columns have an expected part of missings.</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def are_complete(\n    self,\n    columns: list[str],\n    hint: str = \"\",\n    name: str | None = None,\n    where: str | None = None,\n    options: AnalyzerOptions | None = None,\n) -&gt; Self:\n    \"\"\"Add a constraint that columns have an expected part of missings.\"\"\"\n    return self.have_completeness(columns, 1.0, hint, name, where, options)\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.build","title":"<code>build()</code>","text":"<p>Build a Check to the protobuf message.</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def build(self) -&gt; suite.Check:\n    \"\"\"Build a Check to the protobuf message.\"\"\"\n    return suite.Check(\n        checkLevel=self._level,\n        description=self._description,\n        constraints=self._constraints,\n    )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.has_completeness","title":"<code>has_completeness(column, expected_value, hint='', name=None, where=None, options=None)</code>","text":"<p>Add a constraint the the column has an expected part of missings.</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def has_completeness(\n    self,\n    column: str,\n    expected_value: float,\n    hint: str = \"\",\n    name: str | None = None,\n    where: str | None = None,\n    options: AnalyzerOptions | None = None,\n) -&gt; Self:\n    \"\"\"Add a constraint the the column has an expected part of missings.\"\"\"\n    return self.with_constraint(\n        ConstraintBuilder()\n        .for_analyzer(\n            Completeness(\n                column=column,\n                where=where,\n                options=options or AnalyzerOptions.default(),\n            )\n        )\n        .with_hint(hint)\n        .with_name(name or f\"Completeness({column})\")\n        .should_be_eq_to(expected_value)\n        .build()\n    )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.has_size","title":"<code>has_size(expected_size, hint='', name=None)</code>","text":"<p>Add a constraint that the DataFrame has size like expected.</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def has_size(\n    self, expected_size: int, hint: str = \"\", name: str | None = None\n) -&gt; Self:\n    \"\"\"Add a constraint that the DataFrame has size like expected.\"\"\"\n    return self.with_constraint(\n        ConstraintBuilder()\n        .for_analyzer(Size())\n        .with_hint(hint)\n        .with_name(name or \"Size\")\n        .should_be_eq_to(expected_size)\n        .build()\n    )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.has_uniqueness","title":"<code>has_uniqueness(columns, expected_value, hint='', name=None, where=None, options=None)</code>","text":"<p>Create a constraint that the given set of columns have an expected level of uniqueness.</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def has_uniqueness(\n    self,\n    columns: list[str],\n    expected_value: float,\n    hint: str = \"\",\n    name: str | None = None,\n    where: str | None = None,\n    options: AnalyzerOptions | None = None,\n) -&gt; Self:\n    \"\"\"Create a constraint that the given set of columns have an expected level of uniqueness.\"\"\"\n    return self.with_constraint(\n        ConstraintBuilder()\n        .for_analyzer(\n            Uniqueness(\n                columns=columns,\n                where=where,\n                options=options or AnalyzerOptions.default(),\n            )\n        )\n        .with_name(name or f\"Uniquesness{','.join(columns)}\")\n        .with_hint(hint)\n        .should_be_eq_to(expected_value)\n        .build()\n    )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.have_completeness","title":"<code>have_completeness(columns, expected_value, hint='', name=None, where=None, options=None)</code>","text":"<p>Add a constraint that columns have an expected level of completeness.</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def have_completeness(\n    self,\n    columns: list[str],\n    expected_value: float,\n    hint: str = \"\",\n    name: str | None = None,\n    where: str | None = None,\n    options: AnalyzerOptions | None = None,\n) -&gt; Self:\n    \"\"\"Add a constraint that columns have an expected level of completeness.\"\"\"\n    list_of_constraints = [\n        ConstraintBuilder()\n        .for_analyzer(\n            Completeness(\n                column=col,\n                where=where,\n                options=options or AnalyzerOptions.default(),\n            )\n        )\n        .with_name(name or f\"Completeness({col})\")\n        .with_hint(hint)\n        .should_be_eq_to(expected_value)\n        .build()\n        for col in columns\n    ]\n    for constraint in list_of_constraints:\n        self = self.with_constraint(constraint=constraint)\n\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.is_complete","title":"<code>is_complete(column, hint='', name=None, where=None, options=None)</code>","text":"<p>Add a constraint that the column doesn not have missings.</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def is_complete(\n    self,\n    column: str,\n    hint: str = \"\",\n    name: str | None = None,\n    where: str | None = None,\n    options: AnalyzerOptions | None = None,\n) -&gt; Self:\n    \"\"\"Add a constraint that the column doesn not have missings.\"\"\"\n    return self.has_completeness(column, 1.0, hint, name, where, options)\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.is_primary_key","title":"<code>is_primary_key(column, hint='', name=None, where=None, options=None)</code>","text":"<p>Create a contraint that the column is like a primary key.</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def is_primary_key(\n    self,\n    column: str,\n    hint: str = \"\",\n    name: str | None = None,\n    where: str | None = None,\n    options: AnalyzerOptions | None = None,\n) -&gt; Self:\n    \"\"\"Create a contraint that the column is like a primary key.\"\"\"\n    return self.is_unique(column, hint, name, where, options).is_complete(\n        column, hint, name, where, options\n    )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.is_unique","title":"<code>is_unique(column, hint='', name=None, where=None, options=None)</code>","text":"<p>Create a constraint that the column is unique.</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def is_unique(\n    self,\n    column: str,\n    hint: str = \"\",\n    name: str | None = None,\n    where: str | None = None,\n    options: AnalyzerOptions | None = None,\n) -&gt; Self:\n    \"\"\"Create a constraint that the column is unique.\"\"\"\n    return self.has_uniqueness([column], 1.0, hint, name, where, options)\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.with_constraint","title":"<code>with_constraint(constraint)</code>","text":"<p>Add a constraint to the Check.</p> <p>It is recommended to use ConstraintBuilder!</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def with_constraint(self, constraint: suite.Check.Constraint) -&gt; Self:\n    \"\"\"Add a constraint to the Check.\n\n    It is recommended to use ConstraintBuilder!\n    \"\"\"\n    self._constraints.append(constraint)\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.with_constraints","title":"<code>with_constraints(constraints_list)</code>","text":"<p>Set constraints. Override existing!</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def with_constraints(self, constraints_list: list[suite.Check.Constraint]) -&gt; Self:\n    \"\"\"Set constraints. Override existing!\"\"\"\n    self._constraints = constraints_list\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.with_description","title":"<code>with_description(decription)</code>","text":"<p>Set a description of the Check.</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def with_description(self, decription: str) -&gt; Self:\n    \"\"\"Set a description of the Check.\"\"\"\n    self._description = decription\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/checks/#tsumugi-python.tsumugi.checks.CheckBuilder.with_level","title":"<code>with_level(level)</code>","text":"<p>Set a level of the Check.</p> Source code in <code>tsumugi-python/tsumugi/checks.py</code> <pre><code>def with_level(self, level: CheckLevel) -&gt; Self:\n    \"\"\"Set a level of the Check.\"\"\"\n    self._level = level\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/enums/","title":"Enums","text":""},{"location":"python/reference/tsumugi-python/tsumugi/repository/","title":"Repository","text":""},{"location":"python/reference/tsumugi-python/tsumugi/repository/#tsumugi-python.tsumugi.repository.FileSystemRepository","title":"<code>FileSystemRepository</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricRepository</code></p> <p>Represents a FileSystem metric Repository in tsumugi.</p> Source code in <code>tsumugi-python/tsumugi/repository.py</code> <pre><code>@dataclass\nclass FileSystemRepository(MetricRepository):\n    \"\"\"Represents a FileSystem metric Repository in tsumugi.\"\"\"\n\n    path: str\n\n    def _to_proto(self) -&gt; proto.Repository:\n        return proto.Repository(\n            file_system=proto.FileSystemRepository(\n                path=self.path,\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/repository/#tsumugi-python.tsumugi.repository.MetricRepository","title":"<code>MetricRepository</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for all metric repository in tsumugi.</p> Source code in <code>tsumugi-python/tsumugi/repository.py</code> <pre><code>class MetricRepository(ABC):\n    \"\"\"Abstract class for all metric repository in tsumugi.\"\"\"\n\n    @abstractmethod\n    def _to_proto(self) -&gt; proto.Repository: ...\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/repository/#tsumugi-python.tsumugi.repository.SparkTableRepository","title":"<code>SparkTableRepository</code>  <code>dataclass</code>","text":"<p>               Bases: <code>MetricRepository</code></p> <p>Represents a spark table metric repository in tsumugi.</p> Source code in <code>tsumugi-python/tsumugi/repository.py</code> <pre><code>@dataclass\nclass SparkTableRepository(MetricRepository):\n    \"\"\"Represents a spark table metric repository in tsumugi.\"\"\"\n\n    table_name: str\n\n    def _to_proto(self) -&gt; proto.Repository:\n        return proto.Repository(\n            spark_table=proto.SparkTableRepository(\n                table_name=self.table_name,\n            )\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/utils/","title":"Utils","text":""},{"location":"python/reference/tsumugi-python/tsumugi/utils/#tsumugi-python.tsumugi.utils.CheckResult","title":"<code>CheckResult</code>  <code>dataclass</code>","text":"Source code in <code>tsumugi-python/tsumugi/utils.py</code> <pre><code>@dataclass\nclass CheckResult:\n    check: str\n    check_level: str\n    check_status: str\n    constraint: str\n    constraint_status: str\n    constraint_message: str\n\n    @staticmethod\n    def _from_row(row: Row) -&gt; \"CheckResult\":\n        \"\"\"Create an instance from PySpark Row object.\"\"\"\n        return CheckResult(\n            check=row.check,\n            check_level=row.check_level,\n            check_status=row.check_status,\n            constraint=row.constraint,\n            constraint_status=row.constraint_status,\n            constraint_message=row.constraint_message,\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/utils/#tsumugi-python.tsumugi.utils.MetricAndCheckResult","title":"<code>MetricAndCheckResult</code>  <code>dataclass</code>","text":"Source code in <code>tsumugi-python/tsumugi/utils.py</code> <pre><code>@dataclass\nclass MetricAndCheckResult:\n    level: str\n    check_description: str\n    constraint_message: str\n    metric_name: str\n    metric_instance: str\n    metric_entity: str\n    metric_value: float\n    status: str\n    constraint: str\n\n    @staticmethod\n    def _from_row(row: Row) -&gt; \"MetricAndCheckResult\":\n        \"\"\"Create an instance from PySpark Row obejct.\"\"\"\n\n        return MetricAndCheckResult(\n            level=row.level,\n            check_description=row.checkDescription,\n            constraint_message=row.constraintMessage,\n            metric_name=row.metricName,\n            metric_instance=row.metricInstance,\n            metric_entity=row.metricEntity,\n            metric_value=row.metricValue,\n            status=row.status,\n            constraint=row.constraint,\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/utils/#tsumugi-python.tsumugi.utils.MetricResult","title":"<code>MetricResult</code>  <code>dataclass</code>","text":"Source code in <code>tsumugi-python/tsumugi/utils.py</code> <pre><code>@dataclass\nclass MetricResult:\n    entity: str\n    instance: str\n    name: str\n    value: float\n\n    @staticmethod\n    def _from_row(row: Row) -&gt; \"MetricResult\":\n        \"\"\"Create an instance from PySpark Row object.\"\"\"\n        return MetricResult(\n            entity=row.entity,\n            instance=row.instance,\n            name=row.name,\n            value=row.value,\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/","title":"Verification","text":""},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationResult","title":"<code>VerificationResult</code>  <code>dataclass</code>","text":"<p>Results of verification.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>@dataclass\nclass VerificationResult:\n    \"\"\"Results of verification.\"\"\"\n\n    def __init__(self, df: DataFrame | ConnectDataFrame) -&gt; None:\n        \"\"\"This constructor is internal and is not recommended to use.\"\"\"\n        self._has_row_results = ROW_LEVEL_RESULTS_SUB_DF in df.columns\n        self._checks = self._get_checks(df)\n        self._metrics = self._get_metrics(df)\n        self._check_results = self._get_check_results(df)\n        if self._has_row_results:\n            self._row_level_results = self._get_row_level_results(df)\n\n    @property\n    def checks(self) -&gt; tuple[CheckResult]:\n        \"\"\"Results of checks.\n\n        Returns results of all the checks as a collection of dataclasses.\n        Each check can contain multiple Constraints.\n\n        Each check has the following attributes:\n        - check (str): the name of the check\n        - check level (str): the level of the check (Warning, Error)\n        - check status (str): the overall check status (depends of level)\n        - constraint (str): the description of constraint\n        - contraint status (str): the status (Success, Failure)\n        - contraint message (str): resulting message\n        \"\"\"\n        return self._checks\n\n    @property\n    def metrics(self) -&gt; tuple[MetricResult]:\n        \"\"\"Computed metrics.\n\n        Returns all the metrics as a collection of dataclasses.\n\n        Each metric contains:\n        - enitity (str): type of the metric (Dataset, Column)\n        - instance (str): \"*\" in case of Dataset-level metric,\n                          name of the column otherwise\n        - name (str): name of the metric\n        - value (float): the value of the metric\n        \"\"\"\n        return self._metrics\n\n    @property\n    def check_results(self) -&gt; tuple[MetricAndCheckResult]:\n        \"\"\"Results of checks with values of the corresponded metric and constraint.\n\n        Combines results of contraints with the corresponding values\n        and descriptons of metrics. Collection of dataclasses.\n\n        Each element of the collection contains:\n        - level (str): the same as in Check\n        - check description (str): the same as in Check\n        - conatraint message (str): the same as in Check\n        - metric name (str): the name of the related metric\n        - metric instance (str): column name or \"*\"\n        - metric entity (str): Dataset or Column\n        - metric value (str): the value of the related mtric\n        - status (str): Success / Failure\n        - constraint (str): the description of the constraint\n        \"\"\"\n        return self._check_results\n\n    @property\n    def row_level_results(self) -&gt; DataFrame | None:\n        \"\"\"Row-level results as it would be returned by Deequ.\n\n        The original DataFrame and a boolean status column per each Check.\n        \"\"\"\n        if self._has_row_results:\n            return self._row_level_results\n        else:\n            return None\n\n    def checks_as_pandas(self) -&gt; pd.DataFrame:\n        \"\"\"Return checks as a Pandas DataFrame.\n\n        This method construct the new DataFrame each time!\n        If you need it in a loop, it is recommended to cahce an output.\n        \"\"\"\n        return pd.DataFrame.from_records([asdict(val) for val in self.checks])\n\n    def metrics_as_pandas(self) -&gt; pd.DataFrame:\n        \"\"\"Return metrics as a Pandas DataFrame.\n\n        This method construct the new DataFrame each time!\n        If you need it in a loop, it is recommended to cahce an output.\n        \"\"\"\n        return pd.DataFrame.from_records([asdict(val) for val in self.metrics])\n\n    def check_results_as_pandas(self) -&gt; pd.DataFrame:\n        \"\"\"Return check results as a Pandas DataFrame.\n\n        This method construct the new DataFrame each time!\n        If you need it in a loop, it is recommended to cahce an output.\n        \"\"\"\n        return pd.DataFrame.from_records([asdict(val) for val in self.check_results])\n\n    def _get_checks(self, df: DataFrame) -&gt; tuple[CheckResult]:\n        sub_df = df.select(F.explode(F.col(CHECKS_SUB_DF)).alias(\"sub_col\"))\n        collected = sub_df.collect()\n        checks = []\n        for row in collected:\n            sub_row = row.sub_col\n            checks.append(CheckResult._from_row(sub_row))\n\n        return tuple(c for c in checks)\n\n    def _get_metrics(self, df: DataFrame) -&gt; tuple[MetricResult]:\n        sub_df = df.select(F.explode(F.col(METRICS_SUB_DF)).alias(\"sub_col\"))\n        collected = sub_df.collect()\n        metrics = []\n        for row in collected:\n            sub_row = row.sub_col\n            metrics.append(MetricResult._from_row(sub_row))\n\n        return tuple(m for m in metrics)\n\n    def _get_check_results(self, df: DataFrame) -&gt; tuple[MetricAndCheckResult]:\n        sub_df = df.select(F.explode(F.col(CHECK_RESULTS_SUB_DF)).alias(\"sub_col\"))\n        collected = sub_df.collect()\n        metrics_and_checks = []\n        for row in collected:\n            sub_row = row.sub_col\n            metrics_and_checks.append(MetricAndCheckResult._from_row(sub_row))\n\n        return tuple(mc for mc in metrics_and_checks)\n\n    def _get_row_level_results(self, df: DataFrame) -&gt; DataFrame:\n        sub_df = df.select(F.explode(F.col(ROW_LEVEL_RESULTS_SUB_DF)).alias(\"sub_col\"))\n        return sub_df.select(\"sub_col.*\")\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationResult.check_results","title":"<code>check_results: tuple[MetricAndCheckResult]</code>  <code>property</code>","text":"<p>Results of checks with values of the corresponded metric and constraint.</p> <p>Combines results of contraints with the corresponding values and descriptons of metrics. Collection of dataclasses.</p> <p>Each element of the collection contains: - level (str): the same as in Check - check description (str): the same as in Check - conatraint message (str): the same as in Check - metric name (str): the name of the related metric - metric instance (str): column name or \"*\" - metric entity (str): Dataset or Column - metric value (str): the value of the related mtric - status (str): Success / Failure - constraint (str): the description of the constraint</p>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationResult.checks","title":"<code>checks: tuple[CheckResult]</code>  <code>property</code>","text":"<p>Results of checks.</p> <p>Returns results of all the checks as a collection of dataclasses. Each check can contain multiple Constraints.</p> <p>Each check has the following attributes: - check (str): the name of the check - check level (str): the level of the check (Warning, Error) - check status (str): the overall check status (depends of level) - constraint (str): the description of constraint - contraint status (str): the status (Success, Failure) - contraint message (str): resulting message</p>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationResult.metrics","title":"<code>metrics: tuple[MetricResult]</code>  <code>property</code>","text":"<p>Computed metrics.</p> <p>Returns all the metrics as a collection of dataclasses.</p> <p>Each metric contains: - enitity (str): type of the metric (Dataset, Column) - instance (str): \"*\" in case of Dataset-level metric,                   name of the column otherwise - name (str): name of the metric - value (float): the value of the metric</p>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationResult.row_level_results","title":"<code>row_level_results: DataFrame | None</code>  <code>property</code>","text":"<p>Row-level results as it would be returned by Deequ.</p> <p>The original DataFrame and a boolean status column per each Check.</p>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationResult.__init__","title":"<code>__init__(df)</code>","text":"<p>This constructor is internal and is not recommended to use.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def __init__(self, df: DataFrame | ConnectDataFrame) -&gt; None:\n    \"\"\"This constructor is internal and is not recommended to use.\"\"\"\n    self._has_row_results = ROW_LEVEL_RESULTS_SUB_DF in df.columns\n    self._checks = self._get_checks(df)\n    self._metrics = self._get_metrics(df)\n    self._check_results = self._get_check_results(df)\n    if self._has_row_results:\n        self._row_level_results = self._get_row_level_results(df)\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationResult.check_results_as_pandas","title":"<code>check_results_as_pandas()</code>","text":"<p>Return check results as a Pandas DataFrame.</p> <p>This method construct the new DataFrame each time! If you need it in a loop, it is recommended to cahce an output.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def check_results_as_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Return check results as a Pandas DataFrame.\n\n    This method construct the new DataFrame each time!\n    If you need it in a loop, it is recommended to cahce an output.\n    \"\"\"\n    return pd.DataFrame.from_records([asdict(val) for val in self.check_results])\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationResult.checks_as_pandas","title":"<code>checks_as_pandas()</code>","text":"<p>Return checks as a Pandas DataFrame.</p> <p>This method construct the new DataFrame each time! If you need it in a loop, it is recommended to cahce an output.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def checks_as_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Return checks as a Pandas DataFrame.\n\n    This method construct the new DataFrame each time!\n    If you need it in a loop, it is recommended to cahce an output.\n    \"\"\"\n    return pd.DataFrame.from_records([asdict(val) for val in self.checks])\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationResult.metrics_as_pandas","title":"<code>metrics_as_pandas()</code>","text":"<p>Return metrics as a Pandas DataFrame.</p> <p>This method construct the new DataFrame each time! If you need it in a loop, it is recommended to cahce an output.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def metrics_as_pandas(self) -&gt; pd.DataFrame:\n    \"\"\"Return metrics as a Pandas DataFrame.\n\n    This method construct the new DataFrame each time!\n    If you need it in a loop, it is recommended to cahce an output.\n    \"\"\"\n    return pd.DataFrame.from_records([asdict(val) for val in self.metrics])\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationRunBuilder","title":"<code>VerificationRunBuilder</code>","text":"<p>Helper class that simplify building of the Verification Run object.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>class VerificationRunBuilder:\n    \"\"\"Helper class that simplify building of the Verification Run object.\"\"\"\n\n    def __init__(self, df: DataFrame | ConnectDataFrame) -&gt; None:\n        self._data = df\n        self._checks: list[suite.Check] = list()\n        self._required_analyzers: list[AbstractAnalyzer] = list()\n        self._repository: MetricRepository | None = None\n        self._dataset_date: int | None = None\n        self._dataset_tags: dict[str, str] = dict()\n        self._anomaly_detectons: list[suite.AnomalyDetection] = list()\n        self._compute_row_results: bool = False\n\n    def with_row_level_results(self) -&gt; Self:\n        \"\"\"Mark that row-level results should be returned.\"\"\"\n        self._compute_row_results = True\n        return self\n\n    def add_required_analyzer(self, analyzer: AbstractAnalyzer) -&gt; Self:\n        \"\"\"Add a required analyzer metric of that will be computed anyway.\"\"\"\n        self._required_analyzers.append(analyzer)\n        return self\n\n    def add_required_analyzers(self, analyzers: list[AbstractAnalyzer]) -&gt; Self:\n        \"\"\"Set required analyzers. Override existing!\"\"\"\n        self._required_analyzers = analyzers\n        return self\n\n    def add_check(self, check: suite.Check) -&gt; Self:\n        \"\"\"Add a Check object.\n\n        It is recommended to use CheckBuilder!\n        \"\"\"\n        self._checks.append(check)\n        return self\n\n    def add_checks(self, checks: list[suite.Check]) -&gt; Self:\n        \"\"\"Set checks. Override exisitng!\"\"\"\n        self._checks = checks\n        return self\n\n    def with_fs_repository_and_key(\n        self,\n        filepath: str,\n        dataset_date: int,\n        dataset_tags: dict[str, str] | None = None,\n    ) -&gt; Self:\n        \"\"\"Add a FileSystem repository and date and tags for the ResultKey.\"\"\"\n        self._table_name = None\n        self._path = filepath\n        self._dataset_date = dataset_date\n        self._dataset_tags = dataset_tags if dataset_tags is not None else dict()\n        return self\n\n    def with_table_repository_and_key(\n        self,\n        table_name: str,\n        dataset_date: int,\n        dateset_tags: dict[str, str] | None = None,\n    ) -&gt; Self:\n        \"\"\"Add a Table repository and date and tags for the ResultKey.\"\"\"\n        self._path = None\n        self._table_name = table_name\n        self._dataset_date = dataset_date\n        self._dataset_tags = dateset_tags if dateset_tags is not None else dict()\n        return self\n\n    def add_anomaly_detection(self, ad: suite.AnomalyDetection) -&gt; Self:\n        \"\"\"Add an anomaly detection check.\n\n        It is recommended to use AnomalyDetectionBuilder!\n        \"\"\"\n        self._anomaly_detectons.append(ad)\n        return self\n\n    def add_anomaly_detections(self, ads: list[suite.AnomalyDetection]) -&gt; Self:\n        \"\"\"Set anomaly detection checks. Override existing!\"\"\"\n        self._anomaly_detectons = ads\n        return self\n\n    def _validate(self) -&gt; None:\n        if len(self._anomaly_detectons) &gt; 0:\n            if not (self._path or self._table_name):\n                raise ValueError(\"Anomaly detection requires repository and key\")\n\n    def _build(self) -&gt; suite.VerificationSuite:\n        self._validate()\n\n        pb_suite = suite.VerificationSuite(\n            checks=self._checks,\n            required_analyzers=[al._to_proto() for al in self._required_analyzers],\n            compute_row_level_results=self._compute_row_results,\n        )\n\n        if self._repository:\n            pb_suite.repository = self._repository._to_proto()\n            pb_suite.result_key = repository.ResultKey(\n                dataset_date=self._dataset_date, tags=self._dataset_tags\n            )\n\n            for ad in self._anomaly_detectons:\n                pb_suite.anomaly_detections.append(ad)\n\n        return pb_suite\n\n    def run(self) -&gt; VerificationResult:\n        \"\"\"Run the suite.\n\n        The type of runtime is determined by the session attached to the provided DataFrame.\n        For a Spark Connect session, it will add a serialized plan to the Suite and send the message to the Connect Server.\n        For a Spark Classic session, it will directly call the JVM with the suite and a Java DataFrame.\n        \"\"\"\n        spark = self._data.sparkSession\n        pb_suite = self._build()\n        is_classic = (os.environ.get(\"SPARK_CONNECT_MODE_ENABLED\") is None) or hasattr(\n            self._data, \"_jdf\"\n        )\n\n        if is_classic:\n            jvm = spark._jvm\n            jdf = self._data._jdf\n            deequ_jvm_suite = jvm.com.ssinchenko.tsumugi.DeequSuiteBuilder(\n                jdf,\n                pb_suite,\n            )\n            result_jdf = jvm.com.ssinchenko.tsumugi.DeeqUtils.runAndCollectResults(\n                deequ_jvm_suite,\n                spark._jsparkSession,\n                self._compute_row_results,\n                jdf,\n            )\n            return VerificationResult(\n                DataFrame(result_jdf, SQLContext(spark.sparkContext))\n            )\n        else:\n            data_plan = self._data._plan\n            assert data_plan is not None\n            assert isinstance(data_plan, LogicalPlan)\n            pb_suite.data = data_plan.to_proto(spark.client).SerializeToString()\n\n            class DeequSuite(LogicalPlan):\n                def __init__(self, pb_suite: suite.VerificationSuite) -&gt; None:\n                    super().__init__(None)\n                    self._pb_suite = pb_suite\n\n                def plan(self, session: SparkConnectClient) -&gt; Relation:\n                    plan = self._create_proto_relation()\n                    plan.extension.Pack(self._pb_suite)\n                    return plan\n\n            return VerificationResult(\n                ConnectDataFrame.withPlan(DeequSuite(pb_suite=pb_suite), spark)\n            )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationRunBuilder.add_anomaly_detection","title":"<code>add_anomaly_detection(ad)</code>","text":"<p>Add an anomaly detection check.</p> <p>It is recommended to use AnomalyDetectionBuilder!</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def add_anomaly_detection(self, ad: suite.AnomalyDetection) -&gt; Self:\n    \"\"\"Add an anomaly detection check.\n\n    It is recommended to use AnomalyDetectionBuilder!\n    \"\"\"\n    self._anomaly_detectons.append(ad)\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationRunBuilder.add_anomaly_detections","title":"<code>add_anomaly_detections(ads)</code>","text":"<p>Set anomaly detection checks. Override existing!</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def add_anomaly_detections(self, ads: list[suite.AnomalyDetection]) -&gt; Self:\n    \"\"\"Set anomaly detection checks. Override existing!\"\"\"\n    self._anomaly_detectons = ads\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationRunBuilder.add_check","title":"<code>add_check(check)</code>","text":"<p>Add a Check object.</p> <p>It is recommended to use CheckBuilder!</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def add_check(self, check: suite.Check) -&gt; Self:\n    \"\"\"Add a Check object.\n\n    It is recommended to use CheckBuilder!\n    \"\"\"\n    self._checks.append(check)\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationRunBuilder.add_checks","title":"<code>add_checks(checks)</code>","text":"<p>Set checks. Override exisitng!</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def add_checks(self, checks: list[suite.Check]) -&gt; Self:\n    \"\"\"Set checks. Override exisitng!\"\"\"\n    self._checks = checks\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationRunBuilder.add_required_analyzer","title":"<code>add_required_analyzer(analyzer)</code>","text":"<p>Add a required analyzer metric of that will be computed anyway.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def add_required_analyzer(self, analyzer: AbstractAnalyzer) -&gt; Self:\n    \"\"\"Add a required analyzer metric of that will be computed anyway.\"\"\"\n    self._required_analyzers.append(analyzer)\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationRunBuilder.add_required_analyzers","title":"<code>add_required_analyzers(analyzers)</code>","text":"<p>Set required analyzers. Override existing!</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def add_required_analyzers(self, analyzers: list[AbstractAnalyzer]) -&gt; Self:\n    \"\"\"Set required analyzers. Override existing!\"\"\"\n    self._required_analyzers = analyzers\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationRunBuilder.run","title":"<code>run()</code>","text":"<p>Run the suite.</p> <p>The type of runtime is determined by the session attached to the provided DataFrame. For a Spark Connect session, it will add a serialized plan to the Suite and send the message to the Connect Server. For a Spark Classic session, it will directly call the JVM with the suite and a Java DataFrame.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def run(self) -&gt; VerificationResult:\n    \"\"\"Run the suite.\n\n    The type of runtime is determined by the session attached to the provided DataFrame.\n    For a Spark Connect session, it will add a serialized plan to the Suite and send the message to the Connect Server.\n    For a Spark Classic session, it will directly call the JVM with the suite and a Java DataFrame.\n    \"\"\"\n    spark = self._data.sparkSession\n    pb_suite = self._build()\n    is_classic = (os.environ.get(\"SPARK_CONNECT_MODE_ENABLED\") is None) or hasattr(\n        self._data, \"_jdf\"\n    )\n\n    if is_classic:\n        jvm = spark._jvm\n        jdf = self._data._jdf\n        deequ_jvm_suite = jvm.com.ssinchenko.tsumugi.DeequSuiteBuilder(\n            jdf,\n            pb_suite,\n        )\n        result_jdf = jvm.com.ssinchenko.tsumugi.DeeqUtils.runAndCollectResults(\n            deequ_jvm_suite,\n            spark._jsparkSession,\n            self._compute_row_results,\n            jdf,\n        )\n        return VerificationResult(\n            DataFrame(result_jdf, SQLContext(spark.sparkContext))\n        )\n    else:\n        data_plan = self._data._plan\n        assert data_plan is not None\n        assert isinstance(data_plan, LogicalPlan)\n        pb_suite.data = data_plan.to_proto(spark.client).SerializeToString()\n\n        class DeequSuite(LogicalPlan):\n            def __init__(self, pb_suite: suite.VerificationSuite) -&gt; None:\n                super().__init__(None)\n                self._pb_suite = pb_suite\n\n            def plan(self, session: SparkConnectClient) -&gt; Relation:\n                plan = self._create_proto_relation()\n                plan.extension.Pack(self._pb_suite)\n                return plan\n\n        return VerificationResult(\n            ConnectDataFrame.withPlan(DeequSuite(pb_suite=pb_suite), spark)\n        )\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationRunBuilder.with_fs_repository_and_key","title":"<code>with_fs_repository_and_key(filepath, dataset_date, dataset_tags=None)</code>","text":"<p>Add a FileSystem repository and date and tags for the ResultKey.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def with_fs_repository_and_key(\n    self,\n    filepath: str,\n    dataset_date: int,\n    dataset_tags: dict[str, str] | None = None,\n) -&gt; Self:\n    \"\"\"Add a FileSystem repository and date and tags for the ResultKey.\"\"\"\n    self._table_name = None\n    self._path = filepath\n    self._dataset_date = dataset_date\n    self._dataset_tags = dataset_tags if dataset_tags is not None else dict()\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationRunBuilder.with_row_level_results","title":"<code>with_row_level_results()</code>","text":"<p>Mark that row-level results should be returned.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def with_row_level_results(self) -&gt; Self:\n    \"\"\"Mark that row-level results should be returned.\"\"\"\n    self._compute_row_results = True\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationRunBuilder.with_table_repository_and_key","title":"<code>with_table_repository_and_key(table_name, dataset_date, dateset_tags=None)</code>","text":"<p>Add a Table repository and date and tags for the ResultKey.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>def with_table_repository_and_key(\n    self,\n    table_name: str,\n    dataset_date: int,\n    dateset_tags: dict[str, str] | None = None,\n) -&gt; Self:\n    \"\"\"Add a Table repository and date and tags for the ResultKey.\"\"\"\n    self._path = None\n    self._table_name = table_name\n    self._dataset_date = dataset_date\n    self._dataset_tags = dateset_tags if dateset_tags is not None else dict()\n    return self\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationSuite","title":"<code>VerificationSuite</code>","text":"<p>Python-deequ compatibility class.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>class VerificationSuite:\n    \"\"\"Python-deequ compatibility class.\"\"\"\n\n    @staticmethod\n    def on_data(data: DataFrame | ConnectDataFrame) -&gt; VerificationRunBuilder:\n        \"\"\"Return a VerificationRunBuilder for the given DataFrame object.\"\"\"\n        return VerificationRunBuilder(data)\n</code></pre>"},{"location":"python/reference/tsumugi-python/tsumugi/verification/#tsumugi-python.tsumugi.verification.VerificationSuite.on_data","title":"<code>on_data(data)</code>  <code>staticmethod</code>","text":"<p>Return a VerificationRunBuilder for the given DataFrame object.</p> Source code in <code>tsumugi-python/tsumugi/verification.py</code> <pre><code>@staticmethod\ndef on_data(data: DataFrame | ConnectDataFrame) -&gt; VerificationRunBuilder:\n    \"\"\"Return a VerificationRunBuilder for the given DataFrame object.\"\"\"\n    return VerificationRunBuilder(data)\n</code></pre>"}]}